{
  "hash": "dd3b6410783eb1967596dea6465ffaac",
  "result": {
    "markdown": "---\ntitle: \"Appendix 1.B: Analysing global trends in within-country inequality\"\nformat: html\nwarning: false\nfilters:\n  - shinylive\n---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom plotnine import *\n```\n:::\n\n\nLoad in the data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# df_wid = pd.read_csv(\"data/clean/wid.csv\")\n# df_pip = pd.read_csv(\"data/clean/pip.csv\")\n\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf_wid = pd.read_csv(\"PhD_pages/data_appendices/data/clean/wid.csv\")\ndf_pip = pd.read_csv(\"PhD_pages/data_appendices/data/clean/pip.csv\")\n```\n:::\n\n\nA key difficulty of comparing trends is that the data is incomplete coverage. The paper uses two methods to calculate and compare aggregate within-country trends across incomplete data: using reference years and using year fixed effects in a regression.\n\n\n## Method 1: Reference years\n\nI need to allow for the income/consumption issue (currently the data includes both – i.e. multi observations per country year).\n\n\n\n### Mapping data to a reference year\nThis function grabs the observation, by a grouping variable (e.g. by country), for which a reference variable (e.g. year) is closest to a reference value (e.g. 2002).\nYou can specify a maximum distance from the reference value, beyond which no match will be returned.\nBecause there may be tie-breaks – matches equally distant above or below the reference value – there is an argmuent to specify how these tie-breaks should resolved.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#  Function get matching for ref years\ndef closest_to_reference(df, reference_val, max_dist_from_ref, reference_col, group_by_col, value_col, tie_break):\n    \n    df = df.loc[:, [reference_col, group_by_col, value_col]]\n\n    # Drop NAs\n    df = df.dropna()\n\n    # Calculate absolute distance from reference value\n    df['ref_diff'] = abs(df[reference_col] - reference_val)\n\n    # Drop any rows with a distance beyond threshold\n    if not pd.isna(max_dist_from_ref):\n      df = df.loc[df['ref_diff'] <= max_dist_from_ref]\n\n    # Keep closest observation to reference value – including tie-breaks (where there is a match above and below the ref value)\n    df = df[df.groupby(group_by_col)['ref_diff'].transform('min') == df['ref_diff']].reset_index(drop=True)\n\n    # Settle tie-breaks\n    if tie_break == 'below':\n      df = df[df.groupby(group_by_col)[reference_col].transform('min') == df[reference_col]].reset_index(drop=True)\n      \n    elif tie_break == 'above':\n      df = df[df.groupby(group_by_col)[reference_col].transform('max') == df[reference_col]].reset_index(drop=True)\n\n    df = df.drop('ref_diff', axis=1)\n\n    return df\n\n```\n:::\n\n\nI test this function here:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntest = closest_to_reference(\n  df = df_wid, \n  reference_val = 2002,\n  max_dist_from_ref = 5,\n  reference_col = 'Year',\n  group_by_col = 'Entity',\n  value_col = 'Gini',\n  tie_break = 'below'\n  )\n\ntest.head()\n\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=tex}\n\\begin{tabular}{lrlr}\n\\toprule\n{} &  Year &                Entity &      Gini \\\\\n\\midrule\n0 &  1998 &  United Arab Emirates &  0.656802 \\\\\n1 &  2002 &               Albania &  0.481698 \\\\\n2 &  2002 &               Armenia &  0.535265 \\\\\n3 &  2000 &                Angola &  0.682466 \\\\\n4 &  2002 &             Argentina &  0.666745 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n### Obtaining a pair of observations\n\nThis function runs the `closest_to_reference` function twice, over a pair of reference values.\nTie-breaks are settled so as to maximise the gap between the two reference values.\nYou can also specify a minimum distance between the two observations (e.g. pairs of matches that fall less than X years apart will be dropped).\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Merge matches for different reference points\ndef merge_two_ref_matches(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n\n  # Make sure the pair of reference values are in ascending order\n  reference_vals.sort()\n\n  # Maximise distance between two refs by settling tie-breaks below the lowest ref and above the highest ref \n\n  # Find matches for lower reference value\n  lower_ref_matches = closest_to_reference(df, reference_vals[0], max_dist_from_refs[0], reference_col, group_by_col, value_col, 'below')\n\n  # Find matches for higher reference value\n  higher_ref_matches = closest_to_reference(df, reference_vals[1], max_dist_from_refs[1], reference_col, group_by_col, value_col, 'above')\n\n  # Merge the two sets of matches\n  merged_df = pd.merge(lower_ref_matches, higher_ref_matches, on=group_by_col, suffixes=(reference_vals[0], reference_vals[1]))\n\n  # Drop obs that do not have data for both ref values\n  merged_df = merged_df.dropna()\n\n  # Drop obs where the matched data does not meet the min distance requirement\n  if not pd.isna(min_dist_between):\n    \n    # Store the names of the reference column returned from the two matches\n    ref_var_high = f'{reference_col}{reference_vals[1]}'\n    ref_var_low = f'{reference_col}{reference_vals[0]}'\n\n    # Keep only rows >= to the min distance\n    merged_df = merged_df.loc[(merged_df[ref_var_high] - merged_df[ref_var_low]) >= min_dist_between, :]\n\n\n  return merged_df\n\n```\n:::\n\n\nI test this function here:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ntest = merge_two_ref_matches(\n  df = df_wid, \n  reference_vals = [2000, 2010], \n  max_dist_from_refs = [5, 4],\n  min_dist_between = 9,\n  reference_col = 'Year',\n  group_by_col = 'Entity',\n  value_col = 'Gini'\n  )\n\ntest.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=tex}\n\\begin{tabular}{lrlrrr}\n\\toprule\n{} &  Year2000 &                Entity &  Gini2000 &  Year2010 &  Gini2010 \\\\\n\\midrule\n0 &      1998 &  United Arab Emirates &  0.656802 &      2009 &  0.676088 \\\\\n1 &      2002 &               Albania &  0.481698 &      2012 &  0.467515 \\\\\n2 &      1999 &               Armenia &  0.552912 &      2010 &  0.498014 \\\\\n4 &      2001 &             Argentina &  0.660888 &      2010 &  0.570943 \\\\\n6 &      2000 &             Australia &  0.474796 &      2010 &  0.475651 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n#### Allowing for the different welfare concepts in the PIP data\n\nAs noted in .... (add backlink) ... the PIP data contains both income and consumption observations.\nThis function produces matched pairs of observations from that data in which only one pair is selected for each value of the grouping variable (i.e. each country).\nPriority is given to pairs for which both observations relate to income. Second priority is given to pairs where both observations relate to consumption.\nOnly if neither pair is available then it will return a mixed pair of observations if that is available.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# For PIP run this three times – first filtering data for just consumpion only, then with income only, then with a dataset that prefers income over consumption\ndef pip_welfare_routine(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n\n  # Specify the name of the column in which the income/consumption welfare definition is stored\n  welfare_colname = 'welfare_type'\n\n  # Creat dataframes for thee scenarios:\n  # Scenario 1: only allow income data\n  df_inc_filter = df.loc[df[welfare_colname] == \"income\", :]\n  df_inc_filter.name = \"Income\"\n\n  # Scenario 2: only allow consumption data\n  df_cons_filter = df.loc[df[welfare_colname] == \"consumption\", :]\n  df_cons_filter.name = \"Consumption\"\n  # Scenario 3: allow a mix – dropping consumption data where income data is available in the same year\n  df_mixed = df.copy()\n\n  df_mixed['welfare_count'] = df_mixed.groupby([reference_col, group_by_col])[welfare_colname].transform('count')\n\n  df_mixed = df_mixed.loc[(df_mixed['welfare_count'] == 1) | (df_mixed[welfare_colname] == \"income\")]\n\n  df_mixed.name = \"Mixed\"\n  #  Store the scneario dataframes in a list\n  df_scenarios = [df_inc_filter, df_cons_filter, df_mixed]\n\n  # Run the matching function on each scenario\n  scenario_matches = [merge_two_ref_matches(\n    df_scenario, \n    reference_vals, \n    max_dist_from_refs, \n    min_dist_between, \n    reference_col, \n    group_by_col, \n    value_col) for df_scenario in df_scenarios]\n  \n  # Combine the scenarios, keeping only one match where there matches in more than one scenario - in the priority laid out in the df_scenarios list above (income only -> consumption only -> mixed).\n  df_combined_matches = pd.concat([scenario_matches[0], scenario_matches[1]], keys=[df_scenarios[0].name, df_scenarios[1].name])\n\n  # Tidy up indexes\n  df_combined_matches = df_combined_matches.reset_index()\n  \n  df_combined_matches = df_combined_matches.drop('level_1', axis=1)\n\n  df_combined_matches = df_combined_matches\\\n    .rename(columns={\"level_0\": \"pip_welfare\"})\n  return df_combined_matches\n```\n:::\n\n\nI test this function here:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntest = pip_welfare_routine(\n\n  df = df_pip,\n  reference_vals = [1986, 2016], \n  max_dist_from_refs = [5, 5], \n  min_dist_between = 30, \n  reference_col = 'Year',\n  group_by_col = 'Entity',\n  value_col = 'Gini'\n\n  )\n\ntest.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=tex}\n\\begin{tabular}{llrlrrr}\n\\toprule\n{} & pip\\_welfare &  Year1986 &     Entity &  Gini1986 &  Year2016 &  Gini2016 \\\\\n\\midrule\n0 &      Income &      1986 &  Argentina &  0.428089 &      2016 &  0.420325 \\\\\n1 &      Income &      1985 &  Australia &  0.324977 &      2016 &  0.336858 \\\\\n2 &      Income &      1985 &    Belgium &  0.252039 &      2016 &  0.275810 \\\\\n3 &      Income &      1986 &     Brazil &  0.584646 &      2016 &  0.533428 \\\\\n4 &      Income &      1987 &      Chile &  0.562102 &      2017 &  0.444410 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n### Merging reference year aligned data from WID and PIP datasets\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef prep_wid_pip_data(reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n  pip_matches = pip_welfare_routine(\n    df = df_pip,\n    reference_vals = reference_vals,\n    max_dist_from_refs = max_dist_from_refs,\n    min_dist_between = min_dist_between,\n    reference_col = reference_col,\n    group_by_col = group_by_col,\n    value_col = value_col\n  )\n\n  wid_matches = merge_two_ref_matches(\n    df = df_wid,\n    reference_vals = reference_vals,\n    max_dist_from_refs = max_dist_from_refs,\n    min_dist_between = min_dist_between,\n    reference_col = reference_col,\n    group_by_col = group_by_col,\n    value_col = value_col\n  )\n\n  ref_pairs = pd.concat([pip_matches, wid_matches,], keys=['pip', 'wid'])\n\n  # Tidy up indexes\n  ref_pairs = ref_pairs.reset_index()\n  \n  ref_pairs = ref_pairs.drop('level_1', axis=1)\n\n  ref_pairs = ref_pairs\\\n    .rename(columns={\"level_0\": \"source\"})\n\n\n\n  # Add a count by country – showing whether data is available from both sources or not\n  ref_pairs['source_count'] = ref_pairs.groupby('Entity')['source'].transform('count')\n\n  return ref_pairs\n```\n:::\n\n\n## Plot\n\n::: {.cell layout-ncol='2' execution_count=11}\n``` {.python .cell-code}\n# Specifications\nreference_vals = [1990, 2016]\nmax_dist_from_refs = [5, 5]\nmin_dist_between = 25\nvalue_col = 'Gini'\n\n# Set the value which constitutes a significant change in the value being measured\n  # E.g. for Gini tolerance = 0.02 (2 points) \ntolerance = 0.02\n\n# Data from both sources is needed?\n  # If this = 1 then a country will be incldued even if it is only availalbe from one source.\n  # If = 2, then only countries with data for both sources will be included\nsource_count_requirement = 1\n\n# Unlikely to change:\nreference_col = 'Year'\ngroup_by_col = 'Entity'\n\n\n# Prep data\n\nref_pairs = prep_wid_pip_data(\n  reference_vals = reference_vals, \n  max_dist_from_refs = max_dist_from_refs,\n  min_dist_between = min_dist_between,\n  reference_col = reference_col,\n  group_by_col = group_by_col,\n  value_col = value_col,\n)\n\nplot_data = ref_pairs.loc[ref_pairs['source_count'] >= source_count_requirement]\n\n\n# ----- Prep plot ------\n\n# Store the names of the columns to be used onthe X and Y axis\nx_axis = f'{value_col}{reference_vals[0]}'\ny_axis = f'{value_col}{reference_vals[1]}'\n\n# I grab what I am guessing to be the max and min tick marks on the x axist, in order to define the coordinates of a 'tolerance' ribbon.\nx_min = plot_data[x_axis].min()\nx_min_floor = np.floor(x_min * 10) / 10\n\nx_max = plot_data[x_axis].max()\nx_max_ceiling = np.ceil(x_max * 10) / 10\n\n# Set the coordinates of the shaded ribbon\nshaded_coord = pd.DataFrame({'x': [x_min_floor, x_max_ceiling, x_max_ceiling, x_min_floor], \n  'y': [x_min_floor-tolerance, x_max_ceiling-tolerance, x_max_ceiling+tolerance , x_min_floor+tolerance]})\n\n\nplot = (ggplot(plot_data\n, aes(x_axis, y_axis))\n + geom_point()\n + facet_wrap('~ source')\n + geom_polygon(aes(x='x', y='y'), data=shaded_coord, fill=\"#FF000066\"))\n\nplot\n\n# ---- Prep summary table ----\n\n# Count by rise/stable/fall bins\n\n# Calculate the change between the two ref periods\nplot_data['change'] = (plot_data[y_axis] - plot_data[x_axis])\n\n\n# Define the thresholds using the tolerance specified\nthresholds = [-float(\"inf\"), -tolerance, tolerance, float(\"inf\")]\n\n# Count observations in bins defined by the thresholds\nthreshold_counts = plot_data\\\n  .groupby(['source', pd.cut(plot_data['change'], thresholds)])\\\n  .size()\\\n  .unstack()\\\n  .T\\\n\n# Label the bins and set as the index\nthreshold_counts['summary'] = ['fall', 'stable', 'rise']\nthreshold_counts = threshold_counts.set_index('summary')\n\n# Calculate average change and n\nmean_and_count = pd.DataFrame(plot_data.groupby('source')['change']\\\n  .agg(['mean', 'count']))\\\n  .T\n\nmean_and_count['summary'] = ['avg_change', 'n']\n\nmean_and_count = mean_and_count.set_index('summary')\n\n\n# Concat together and present in a table\n\ndf_summary = pd.concat([threshold_counts, mean_and_count])\n\ndf_summary\n\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=tex}\n\\begin{tabular}{lrr}\n\\toprule\nsource &        pip &        wid \\\\\nsummary    &            &            \\\\\n\\midrule\nfall       &  23.000000 &  13.000000 \\\\\nstable     &  12.000000 &  12.000000 \\\\\nrise       &  23.000000 &  39.000000 \\\\\navg\\_change &  -0.011373 &   0.037544 \\\\\nn          &  58.000000 &  64.000000 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n## Method 2: regression analysis\n\n\n## Explore the date\n\nI plan to build a Shiny app to help compare trends across datasets (using Shinylive – built on Shiny for Python - ).\n\nHere is a test app just so I can test the wiring of how such an app works.\n\n\n```{shinylive-python}\n#| standalone: true\n#| viewerHeight: 420\n\nfrom shiny import *\nfrom plotnine import *\nfrom pyodide.http import open_url\nimport pandas as pd\n\n\n\napp_ui = ui.page_fluid(\n    ui.output_plot(\"example_plot\"),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def example_plot():\n    \n        url = 'https://raw.githubusercontent.com/owid/notebooks/main/BetterDataDocs/JoeHasell/PIP/data/ppp_2017/final/PIP_data_public_download/full_dataset/cons_only/poverty_cons_only.csv'\n        \n        df = pd.read_csv(open_url(url))\n\n        plot = (ggplot(df, aes('Year', 'headcount_ratio_365', color = 'Entity'))\n        + geom_line())\n\n        # d = {'col1': [1, 2], 'col2': [3, 4]}\n        # df = pd.DataFrame(data=d)\n        # plot = (ggplot(df, aes('col1', 'col2'))\n        # + geom_point())\n\n\n        return plot\n\n\napp = App(app_ui, server)\n\n```\n\n",
    "supporting": [
      "1_B_within_country_trends_files"
    ],
    "filters": []
  }
}