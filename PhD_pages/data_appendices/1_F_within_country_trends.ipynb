{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Appendix 1.F: Analysing global trends in within-country inequality\"\n",
        "format: html\n",
        "warning: false\n",
        "filters:\n",
        "  - shinylive\n",
        "\n",
        "---"
      ],
      "id": "f6dfb998"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from plotnine import *\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "id": "317339a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This first block of code.....\n"
      ],
      "id": "06807273"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#  ---------------------------------\n",
        "#  ---------- SECTION 1: Data prep functions -------------\n",
        "#  ----------------------------------\n",
        "\n",
        "#  Function get matching for ref years\n",
        "def closest_to_reference(df, reference_val, max_dist_from_ref, reference_col, group_by_col, value_col, tie_break):\n",
        "  \n",
        "  df = df.loc[:, [reference_col, group_by_col, value_col]]\n",
        "\n",
        "  # Drop NAs\n",
        "  df = df.dropna()\n",
        "\n",
        "  # Calculate absolute distance from reference value\n",
        "  df['ref_diff'] = abs(df[reference_col] - reference_val)\n",
        "\n",
        "  # Drop any rows with a distance beyond threshold\n",
        "  if not pd.isna(max_dist_from_ref):\n",
        "    df = df.loc[df['ref_diff'] <= max_dist_from_ref]\n",
        "\n",
        "  # Keep closest observation to reference value – including tie-breaks (where there is a match above and below the ref value)\n",
        "  df = df[df.groupby(group_by_col)['ref_diff'].transform('min') == df['ref_diff']].reset_index(drop=True)\n",
        "\n",
        "  # Settle tie-breaks\n",
        "  if tie_break == 'below':\n",
        "    df = df[df.groupby(group_by_col)[reference_col].transform('min') == df[reference_col]].reset_index(drop=True)\n",
        "    \n",
        "  elif tie_break == 'above':\n",
        "    df = df[df.groupby(group_by_col)[reference_col].transform('max') == df[reference_col]].reset_index(drop=True)\n",
        "\n",
        "  df = df.drop('ref_diff', axis=1)\n",
        "\n",
        "  df = df\\\n",
        "    .rename(columns={value_col: \"value\"})\n",
        "\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Merge matches for different reference points\n",
        "def merge_two_ref_matches(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n",
        "\n",
        "# Make sure the pair of reference values are in ascending order\n",
        "  reference_vals.sort()\n",
        "\n",
        "# Maximise distance between two refs by settling tie-breaks below the lowest ref and above the highest ref \n",
        "\n",
        "# Find matches for lower reference value\n",
        "  lower_ref_matches = closest_to_reference(df, reference_vals[0], max_dist_from_refs, reference_col, group_by_col, value_col, 'below')\n",
        "\n",
        "# Find matches for higher reference value\n",
        "  higher_ref_matches = closest_to_reference(df, reference_vals[1], max_dist_from_refs, reference_col, group_by_col, value_col, 'above')\n",
        "\n",
        "# Merge the two sets of matches\n",
        "  merged_df = pd.merge(lower_ref_matches, higher_ref_matches, on=group_by_col, suffixes=(reference_vals[0], reference_vals[1]))\n",
        "\n",
        "# Drop obs that do not have data for both ref values\n",
        "  merged_df = merged_df.dropna()\n",
        "\n",
        "# Drop obs where the matched data does not meet the min distance requirement\n",
        "  if not pd.isna(min_dist_between):\n",
        "  \n",
        "  # Store the names of the reference column returned from the two matches\n",
        "    ref_var_high = f'{reference_col}{reference_vals[1]}'\n",
        "    ref_var_low = f'{reference_col}{reference_vals[0]}'\n",
        "\n",
        "  # Keep only rows >= to the min distance\n",
        "    merged_df = merged_df.loc[(merged_df[ref_var_high] - merged_df[ref_var_low]) >= min_dist_between, :]\n",
        "\n",
        "\n",
        "\n",
        "  return merged_df\n",
        "\n",
        "\n",
        "\n",
        "  # For PIP run this three times – first filtering data for just consumpion only, then with income only, then with a dataset that prefers income over consumption\n",
        "def pip_welfare_routine(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n",
        "\n",
        "    # Specify the name of the column in which the income/consumption welfare definition is stored\n",
        "    welfare_colname = 'welfare_type'\n",
        "\n",
        "    # Creat dataframes for thee scenarios:\n",
        "    # Scenario 1: only allow income data\n",
        "    df_inc_filter = df.loc[df[welfare_colname] == \"income\", :]\n",
        "    df_inc_filter.name = \"Income\"\n",
        "\n",
        "    # Scenario 2: only allow consumption data\n",
        "    df_cons_filter = df.loc[df[welfare_colname] == \"consumption\", :]\n",
        "    df_cons_filter.name = \"Consumption\"\n",
        "    # Scenario 3: allow a mix – dropping consumption data where income data is available in the same year\n",
        "    df_mixed = df.copy()\n",
        "\n",
        "    df_mixed['welfare_count'] = df_mixed.groupby([reference_col, group_by_col])[welfare_colname].transform('count')\n",
        "\n",
        "    df_mixed = df_mixed.loc[(df_mixed['welfare_count'] == 1) | (df_mixed[welfare_colname] == \"income\")]\n",
        "\n",
        "    df_mixed.name = \"Mixed\"\n",
        "    #  Store the scneario dataframes in a list\n",
        "    df_scenarios = [df_inc_filter, df_cons_filter, df_mixed]\n",
        "\n",
        "    # Run the matching function on each scenario\n",
        "    scenario_matches = [merge_two_ref_matches(\n",
        "      df_scenario, \n",
        "      reference_vals, \n",
        "      max_dist_from_refs, \n",
        "      min_dist_between, \n",
        "      reference_col, \n",
        "      group_by_col, \n",
        "      value_col) for df_scenario in df_scenarios]\n",
        "    \n",
        "    # Combine the first two scenarios.\n",
        "    df_combined_matches = pd.concat([scenario_matches[0], scenario_matches[1]], keys=[df_scenarios[0].name, df_scenarios[1].name])\n",
        "\n",
        "    # Tidy up indexes\n",
        "    df_combined_matches = df_combined_matches.reset_index()\n",
        "    \n",
        "    df_combined_matches = df_combined_matches.drop('level_1', axis=1)\n",
        "\n",
        "    df_combined_matches = df_combined_matches\\\n",
        "      .rename(columns={\"level_0\": \"pip_welfare\"})\n",
        "\n",
        "    # Add in third scenario.\n",
        "    df_combined_matches = pd.concat([df_combined_matches, scenario_matches[2]])\n",
        "\n",
        "    # add scenario name to te pip_welfare column\n",
        "    df_combined_matches['pip_welfare'] = df_combined_matches['pip_welfare'].fillna(df_scenarios[2].name)\n",
        "\n",
        "    # Keep only one match per group (e.g. per Country) - in the priority laid out in the df_scenarios list above (income only -> consumption only -> mixed)\n",
        "      # First count the matches\n",
        "    df_combined_matches['match_count'] = df_combined_matches.groupby(group_by_col)['pip_welfare'].transform('count')\n",
        "      # Then drop any matches from the lowest priority where there are multiple matches\n",
        "    df_combined_matches = df_combined_matches.loc[(df_combined_matches['match_count']==1) | ~(df_combined_matches['pip_welfare']==df_scenarios[2].name)]\n",
        "      #  Repeat at the next level of priority\n",
        "    df_combined_matches['match_count'] = df_combined_matches.groupby(group_by_col)['pip_welfare'].transform('count')\n",
        "    df_combined_matches = df_combined_matches.loc[(df_combined_matches['match_count']==1) | ~(df_combined_matches['pip_welfare']==df_scenarios[1].name)]\n",
        "    \n",
        "    # Drop the match count column\n",
        "    df_combined_matches = df_combined_matches.drop('match_count', axis=1)\n",
        "\n",
        "\n",
        "    return df_combined_matches\n"
      ],
      "id": "fadc3de0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This second block of code we read in the data (this needs to be done in a different way in the shiny app...)\n"
      ],
      "id": "a595d42c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#  ---------------------------------\n",
        "#  ---------- SECTION 2: Read in data -------------\n",
        "#  ----------------------------------\n",
        "\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/pip.csv'\n",
        "        \n",
        "df_pip = pd.read_csv(url)\n",
        "\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/wid.csv'\n",
        "\n",
        "df_wid = pd.read_csv(url)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/region_mapping.csv'\n",
        "\n",
        "df_regions = pd.read_csv(url)\n",
        "\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/population.csv'\n",
        "\n",
        "df_pop = pd.read_csv(url)\n"
      ],
      "id": "b1130066",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a third block, we set specifications for the data matching. This is different in the app because some parameters are controlled by the app.\n"
      ],
      "id": "be102982"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#  ---------------------------------\n",
        "#  ---------- SECTION 3: Specifications -------------\n",
        "#  ----------------------------------\n",
        "\n",
        "# These parameters are not controlled in the app.\n",
        "reference_col = 'Year'\n",
        "group_by_col = 'Entity'\n",
        "\n",
        "tolerance = 1\n",
        "outlier_cut_off_upper = None\n",
        "\n",
        "source_count_requirement = \"all\"\n",
        "# source_count_requirement = \"any\"\n",
        "\n",
        "\n",
        "\n",
        "reference_vals = [1990,2018]\n",
        "max_dist_from_refs = 4\n",
        "min_dist_between = 5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = {\"PIP\": df_pip,\n",
        "        \"WID\": df_wid}\n",
        "\n",
        "\n",
        "data_to_match = {\n",
        "  \"sources\": [\"PIP\", \"WID\", \"PIP\"],\n",
        "  \"var_names\": [\"Gini\", \"Top_10_share\", \"Top_10_share\"]\n",
        "  }\n"
      ],
      "id": "6f992898",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this code block we run the code and prepare final plots and tables.\n"
      ],
      "id": "1b727309"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#  ---------------------------------\n",
        "#  ---------- SECTION 4: Run data functions and produce plot -------------\n",
        "#  ----------------------------------\n",
        "\n",
        "\n",
        "# Combine the source and varname strings element-wise to produce keys that will be used for the concat later\n",
        "source_var_keys = [i + j for i, j in zip(data_to_match[\"sources\"], data_to_match[\"var_names\"])]\n",
        "\n",
        "\n",
        "def prep_and_merge_multiple_vars(data_to_match, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col):\n",
        "  \n",
        "  matches = []\n",
        "\n",
        "  for i in range(len(data_to_match['sources'])):\n",
        "    \n",
        "    source = data_to_match[\"sources\"][i]\n",
        "    var = data_to_match[\"var_names\"][i]\n",
        "    print(i)\n",
        "    print(source)\n",
        "    print(var)\n",
        "    df = data[source]\n",
        "\n",
        "    if source == \"PIP\":\n",
        "\n",
        "      matched = pip_welfare_routine(\n",
        "          df = df,\n",
        "          reference_vals = reference_vals,\n",
        "          max_dist_from_refs = max_dist_from_refs,\n",
        "          min_dist_between = min_dist_between,\n",
        "          reference_col = reference_col,\n",
        "          group_by_col = group_by_col,\n",
        "          value_col = var\n",
        "        )\n",
        "\n",
        "    else:\n",
        "\n",
        "      matched = merge_two_ref_matches(\n",
        "          df = df,\n",
        "          reference_vals = reference_vals,\n",
        "          max_dist_from_refs = max_dist_from_refs,\n",
        "          min_dist_between = min_dist_between,\n",
        "          reference_col = reference_col,\n",
        "          group_by_col = group_by_col,\n",
        "          value_col = var\n",
        "        )\n",
        "\n",
        "    matches.append(matched)\n",
        "\n",
        "\n",
        "  ref_matches = pd.concat(matches, keys=source_var_keys)\n",
        "\n",
        "  # Tidy up indexes\n",
        "  ref_matches = ref_matches.reset_index()\n",
        "  \n",
        "  ref_matches = ref_matches.drop('level_1', axis=1)\n",
        "\n",
        "  ref_matches = ref_matches\\\n",
        "    .rename(columns={\"level_0\": \"source_var\"})\n",
        "\n",
        "  return ref_matches\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prep_plot_and_tables(\n",
        "  data_to_match,\n",
        "  reference_vals, \n",
        "  max_dist_from_refs,\n",
        "  min_dist_between,\n",
        "  reference_col,\n",
        "  group_by_col,\n",
        "  tolerance,\n",
        "  outlier_cut_off_upper,\n",
        "  source_count_requirement):\n",
        "\n",
        "  # Other spec:\n",
        "  region_col = 'region_alt'\n",
        "\n",
        "  # ---- Prep data ---------\n",
        "\n",
        "  plot_data = prep_and_merge_multiple_vars(\n",
        "    data_to_match = data_to_match,\n",
        "    reference_vals = reference_vals, \n",
        "    max_dist_from_refs = max_dist_from_refs,\n",
        "    min_dist_between = min_dist_between,\n",
        "    reference_col = reference_col,\n",
        "    group_by_col = group_by_col\n",
        "  )\n",
        "\n",
        "  # Store the names of the columns to be used onthe X and Y axis\n",
        "  x_axis = f'value{reference_vals[0]}'\n",
        "  y_axis = f'value{reference_vals[1]}'\n",
        "\n",
        "  x_ref_val = f'{reference_col}{reference_vals[0]}'\n",
        "  y_ref_val = f'{reference_col}{reference_vals[1]}'\n",
        "\n",
        "\n",
        "  # Apply outlier cut off, if specified\n",
        "  if not pd.isna(outlier_cut_off_upper):\n",
        "    plot_data = plot_data.loc[plot_data[x_axis] <= outlier_cut_off_upper]\n",
        "    plot_data = plot_data.loc[plot_data[y_axis] <= outlier_cut_off_upper]\n",
        "\n",
        "\n",
        "  # Add a count by country – counting whether data is available from each source or not\n",
        "  plot_data['source_count'] = plot_data.groupby(group_by_col)['source_var'].transform('count')\n",
        "\n",
        "\n",
        "  # Apply source requirement (whether to include only observations with data from all sources)\n",
        "  if source_count_requirement == \"all\":\n",
        "    plot_data = plot_data.loc[plot_data['source_count'] == plot_data['source_count'].max()]\n",
        "\n",
        "  # Drop source_count column\n",
        "  plot_data = plot_data.drop('source_count', axis=1)\n",
        "\n",
        "  # Add in region classification\n",
        "  plot_data = pd.merge(plot_data, df_regions, how = 'left')\n",
        "\n",
        "  # Add in population data\n",
        "    # For first ref\n",
        "  df_pop_ref = df_pop.loc[df_pop['Year'] == reference_vals[0], ['Entity', 'population'] ]\n",
        "\n",
        "  plot_data = pd.merge(plot_data, df_pop_ref, how = 'left')\n",
        "\n",
        "  plot_data = plot_data.rename(columns={'population':'population_ref1'})\n",
        "\n",
        "    # For second ref\n",
        "  df_pop_ref = df_pop.loc[df_pop['Year'] == reference_vals[1], ['Entity', 'population'] ]\n",
        "\n",
        "  plot_data = pd.merge(plot_data, df_pop_ref, how = 'left')\n",
        "\n",
        "  plot_data = plot_data.rename(columns={'population':'population_ref2'})\n",
        "\n",
        "    # Calculate average population two reference periods\n",
        "  plot_data['avg_pop'] = (plot_data['population_ref1'] + plot_data['population_ref2'])/2\n",
        "\n",
        "\n",
        "\n",
        "  # ----- Prep plot ------\n",
        "\n",
        "  # I grab what I am guessing to be the max and min tick marks on the x axist, in order to define the coordinates of a 'tolerance' ribbon.\n",
        "  x_min = plot_data[x_axis].min()\n",
        "  x_min_floor = np.floor(x_min * 10) / 10\n",
        "\n",
        "  x_max = plot_data[x_axis].max()\n",
        "  x_max_ceiling = np.ceil(x_max * 10) / 10\n",
        "\n",
        "  # Set the coordinates of the shaded ribbon\n",
        "  shaded_coord = pd.DataFrame({'x': [x_min_floor, x_max_ceiling, x_max_ceiling, x_min_floor], \n",
        "    'y': [x_min_floor-tolerance, x_max_ceiling-tolerance, x_max_ceiling+tolerance , x_min_floor+tolerance]})\n",
        "\n",
        "\n",
        "  plot = (ggplot(plot_data\n",
        "  , aes(x_axis, y_axis, alpha=0.5))\n",
        "  + geom_point(aes(color=region_col, size='avg_pop', alpha = 0.6))\n",
        "  + facet_wrap('~ source_var')\n",
        "  + geom_polygon(aes(x='x', y='y', alpha = 0.2), data=shaded_coord)\n",
        "  + theme_light()\n",
        "  + theme(legend_position='none')\n",
        "  )\n",
        "\n",
        "\n",
        "  # ---- Prep summary table ----\n",
        "\n",
        "\n",
        "  # Calculate the change between the two ref periods\n",
        "  plot_data['change'] = (plot_data[y_axis] - plot_data[x_axis])\n",
        "\n",
        "\n",
        "  group_yes_vars = ['source_var', region_col]\n",
        "  group_no_vars = ['source_var']\n",
        "\n",
        "  group_scenarios = [group_no_vars, group_yes_vars] \n",
        "\n",
        "  aggs = []\n",
        "  \n",
        "  for group_vars in group_scenarios:\n",
        "  \n",
        "  # Prepare pop-weighted average change\n",
        "    agg_level = plot_data.copy()\n",
        "\n",
        "    # Calulate pop weights by source and region\n",
        "    agg_level['regional_pop_weights'] = agg_level.groupby(group_vars)['avg_pop'].transform(lambda x: x/x.sum()) \n",
        "\n",
        "    agg_level['global_pop_weights'] = agg_level.groupby('source_var')['avg_pop'].transform(lambda x: x/x.sum()) \n",
        "\n",
        "    # Multiply the change by the pop weights \n",
        "    agg_level['region_weighted_change'] = agg_level['change'] * agg_level['regional_pop_weights']\n",
        "\n",
        "\n",
        "  # Prepare fall/stable/rise categories\n",
        "    agg_level['fall'] = agg_level['change'] < -tolerance\n",
        "\n",
        "    agg_level['stable'] = (agg_level['change'] <= tolerance) & (agg_level['change'] >= -tolerance)\n",
        "\n",
        "    agg_level['rise'] = agg_level['change'] > tolerance\n",
        "\n",
        "    agg_level[['fall', 'stable', 'rise']] = agg_level[['fall', 'stable', 'rise']].astype(int)\n",
        "\n",
        "    agg_level = agg_level.groupby(group_vars)[['change', 'region_weighted_change', 'global_pop_weights', 'fall', 'stable', 'rise']]\\\n",
        "      .agg(['sum', 'mean', 'count'])\n",
        "\n",
        "  # Aggregate by source and region\n",
        "    if group_vars == group_yes_vars:\n",
        "\n",
        "      agg_level = agg_level\\\n",
        "        .unstack().T.reset_index()\n",
        "\n",
        "    elif group_vars == group_no_vars: \n",
        "\n",
        "      agg_level = agg_level\\\n",
        "        .T.reset_index()\n",
        "\n",
        "      agg_level[region_col] = \"World\"\n",
        "\n",
        "    aggs.append(agg_level) \n",
        "\n",
        "  \n",
        "  df_summary = pd.concat(aggs)\n",
        "\n",
        "    # filter for the aggregations we need and label in the 'summary' column\n",
        "  df_summary.loc[(df_summary['level_0'] == 'change') & (df_summary['level_1'] == 'mean'), 'summary'] = 'avg change'\n",
        "\n",
        "  df_summary.loc[(df_summary['level_0'] == 'region_weighted_change') & (df_summary['level_1'] == 'sum'), 'summary'] = 'Regional pop-weighted avg change'\n",
        "\n",
        "  df_summary.loc[(df_summary['level_0'] == 'global_pop_weights') & (df_summary['level_1'] == 'sum'), 'summary'] = 'Global pop weights'\n",
        "\n",
        "  df_summary.loc[(df_summary['level_0'] == 'change') & (df_summary['level_1'] == 'count'), 'summary'] = 'n'\n",
        "\n",
        "  df_summary.loc[(df_summary['level_0'] == 'fall') & (df_summary['level_1'] == 'sum'), 'summary'] = 'fall'\n",
        "\n",
        "  df_summary.loc[(df_summary['level_0'] == 'stable') & (df_summary['level_1'] == 'sum'), 'summary'] = 'stable'\n",
        "\n",
        "  df_summary.loc[(df_summary['level_0'] == 'rise') & (df_summary['level_1'] == 'sum'), 'summary'] = 'rise'\n",
        "\n",
        "  df_summary = df_summary[df_summary['summary'].notnull()]\n",
        "\n",
        "\n",
        "  df_summary = df_summary.drop(['level_0', 'level_1'], axis = 1)\n",
        "\n",
        "\n",
        "  df_summary['summary'] = pd.Categorical(df_summary['summary'], categories=['fall', 'stable', 'rise','n', 'avg change', 'Regional pop-weighted avg change', 'Global pop weights'], ordered=True)\n",
        "\n",
        "\n",
        "  df_summary = df_summary.sort_values([region_col,'summary']).set_index([region_col,'summary'])\n",
        "  \n",
        "  \n",
        "  # format to show appropriate d.p. I run a loop over each row for want of figuring out a better way!\n",
        "  repeat_length = 7\n",
        "  for i in range(0, len(df_summary.index.levels[0].unique())):\n",
        "    df_summary.iloc[i*repeat_length,:] = df_summary.iloc[i*repeat_length,:].map('{:.0f}'.format)\n",
        "    df_summary.iloc[1 + i*repeat_length,:] = df_summary.iloc[1 + i*repeat_length,:].map('{:.0f}'.format)\n",
        "    df_summary.iloc[2 + i*repeat_length,:] = df_summary.iloc[2 + i*repeat_length,:].map('{:.0f}'.format)\n",
        "    df_summary.iloc[3 + i*repeat_length,:] = df_summary.iloc[3 + i*repeat_length,:].map('{:.0f}'.format)\n",
        "    df_summary.iloc[4 + i*repeat_length,:] = df_summary.iloc[4 + i*repeat_length,:].map('{:.2f}'.format)\n",
        "    df_summary.iloc[5 + i*repeat_length,:] = df_summary.iloc[5 + i*repeat_length,:].map('{:.2f}'.format)\n",
        "    df_summary.iloc[6 + i*repeat_length,:] = df_summary.iloc[6 + i*repeat_length,:].map('{:.2f}'.format)\n",
        "\n",
        "\n",
        "\n",
        "  # ----- Return outputs ------\n",
        "  plot_and_tables = {\n",
        "    \"table_data\": plot_data,\n",
        "    \"plot\": plot,\n",
        "    \"summary\": df_summary\n",
        "  }\n",
        "  \n",
        "  return plot_and_tables\n"
      ],
      "id": "fa1f59df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_and_tables = prep_plot_and_tables(\n",
        "    data_to_match = data_to_match,\n",
        "    reference_vals = reference_vals, \n",
        "    max_dist_from_refs = max_dist_from_refs,\n",
        "    min_dist_between = min_dist_between,\n",
        "    reference_col = reference_col,\n",
        "    group_by_col = group_by_col,\n",
        "    tolerance = tolerance,\n",
        "    outlier_cut_off_upper = outlier_cut_off_upper,\n",
        "    source_count_requirement = source_count_requirement\n",
        "    )\n",
        "\n",
        "plot = plot_and_tables['plot']\n",
        "summary = plot_and_tables['summary']\n"
      ],
      "id": "bfb19af8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot"
      ],
      "id": "14d8ba15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summary"
      ],
      "id": "5d1cbb58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore the data\n",
        "\n",
        "I plan to build a Shiny app to help compare trends across datasets (using Shinylive – built on Shiny for Python - ).\n",
        "\n",
        "Here is a test app just so I can test the wiring of how such an app works.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```{shinylive-python}\n",
        "# #| standalone: true\n",
        "# #| viewerHeight: 420\n",
        "\n",
        "# from shiny import *\n",
        "# from plotnine import *\n",
        "# from pyodide.http import open_url\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #  ---------------------------------\n",
        "# #  ---------- Data prep functions -------------\n",
        "# #  ----------------------------------\n",
        "\n",
        "# #  Function get matching for ref years\n",
        "# def closest_to_reference(df, reference_val, max_dist_from_ref, reference_col, group_by_col, value_col, tie_break):\n",
        "  \n",
        "#   df = df.loc[:, [reference_col, group_by_col, value_col]]\n",
        "\n",
        "#   # Drop NAs\n",
        "#   df = df.dropna()\n",
        "\n",
        "#   # Calculate absolute distance from reference value\n",
        "#   df['ref_diff'] = abs(df[reference_col] - reference_val)\n",
        "\n",
        "#   # Drop any rows with a distance beyond threshold\n",
        "#   if not pd.isna(max_dist_from_ref):\n",
        "#     df = df.loc[df['ref_diff'] <= max_dist_from_ref]\n",
        "\n",
        "#   # Keep closest observation to reference value – including tie-breaks (where there is a match above and below the ref value)\n",
        "#   df = df[df.groupby(group_by_col)['ref_diff'].transform('min') == df['ref_diff']].reset_index(drop=True)\n",
        "\n",
        "#   # Settle tie-breaks\n",
        "#   if tie_break == 'below':\n",
        "#     df = df[df.groupby(group_by_col)[reference_col].transform('min') == df[reference_col]].reset_index(drop=True)\n",
        "    \n",
        "#   elif tie_break == 'above':\n",
        "#     df = df[df.groupby(group_by_col)[reference_col].transform('max') == df[reference_col]].reset_index(drop=True)\n",
        "\n",
        "#   df = df.drop('ref_diff', axis=1)\n",
        "\n",
        "#   return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Merge matches for different reference points\n",
        "# def merge_two_ref_matches(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n",
        "\n",
        "# # Make sure the pair of reference values are in ascending order\n",
        "#   reference_vals.sort()\n",
        "\n",
        "# # Maximise distance between two refs by settling tie-breaks below the lowest ref and above the highest ref \n",
        "\n",
        "# # Find matches for lower reference value\n",
        "#   lower_ref_matches = closest_to_reference(df, reference_vals[0], max_dist_from_refs, reference_col, group_by_col, value_col, 'below')\n",
        "\n",
        "# # Find matches for higher reference value\n",
        "#   higher_ref_matches = closest_to_reference(df, reference_vals[1], max_dist_from_refs, reference_col, group_by_col, value_col, 'above')\n",
        "\n",
        "# # Merge the two sets of matches\n",
        "#   merged_df = pd.merge(lower_ref_matches, higher_ref_matches, on=group_by_col, suffixes=(reference_vals[0], reference_vals[1]))\n",
        "\n",
        "# # Drop obs that do not have data for both ref values\n",
        "#   merged_df = merged_df.dropna()\n",
        "\n",
        "# # Drop obs where the matched data does not meet the min distance requirement\n",
        "#   if not pd.isna(min_dist_between):\n",
        "  \n",
        "#   # Store the names of the reference column returned from the two matches\n",
        "#     ref_var_high = f'{reference_col}{reference_vals[1]}'\n",
        "#     ref_var_low = f'{reference_col}{reference_vals[0]}'\n",
        "\n",
        "#   # Keep only rows >= to the min distance\n",
        "#     merged_df = merged_df.loc[(merged_df[ref_var_high] - merged_df[ref_var_low]) >= min_dist_between, :]\n",
        "\n",
        "\n",
        "#   return merged_df\n",
        "\n",
        "\n",
        "\n",
        "#   # For PIP run this three times – first filtering data for just consumpion only, then with income only, then with a dataset that prefers income over consumption\n",
        "# def pip_welfare_routine(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n",
        "\n",
        "#     # Specify the name of the column in which the income/consumption welfare definition is stored\n",
        "#     welfare_colname = 'welfare_type'\n",
        "\n",
        "#     # Creat dataframes for thee scenarios:\n",
        "#     # Scenario 1: only allow income data\n",
        "#     df_inc_filter = df.loc[df[welfare_colname] == \"income\", :]\n",
        "#     df_inc_filter.name = \"Income\"\n",
        "\n",
        "#     # Scenario 2: only allow consumption data\n",
        "#     df_cons_filter = df.loc[df[welfare_colname] == \"consumption\", :]\n",
        "#     df_cons_filter.name = \"Consumption\"\n",
        "#     # Scenario 3: allow a mix – dropping consumption data where income data is available in the same year\n",
        "#     df_mixed = df.copy()\n",
        "\n",
        "#     df_mixed['welfare_count'] = df_mixed.groupby([reference_col, group_by_col])[welfare_colname].transform('count')\n",
        "\n",
        "#     df_mixed = df_mixed.loc[(df_mixed['welfare_count'] == 1) | (df_mixed[welfare_colname] == \"income\")]\n",
        "\n",
        "#     df_mixed.name = \"Mixed\"\n",
        "#     #  Store the scneario dataframes in a list\n",
        "#     df_scenarios = [df_inc_filter, df_cons_filter, df_mixed]\n",
        "\n",
        "#     # Run the matching function on each scenario\n",
        "#     scenario_matches = [merge_two_ref_matches(\n",
        "#       df_scenario, \n",
        "#       reference_vals, \n",
        "#       max_dist_from_refs, \n",
        "#       min_dist_between, \n",
        "#       reference_col, \n",
        "#       group_by_col, \n",
        "#       value_col) for df_scenario in df_scenarios]\n",
        "    \n",
        "#     # Combine the first two scenarios.\n",
        "#     df_combined_matches = pd.concat([scenario_matches[0], scenario_matches[1]], keys=[df_scenarios[0].name, df_scenarios[1].name])\n",
        "\n",
        "#     # Tidy up indexes\n",
        "#     df_combined_matches = df_combined_matches.reset_index()\n",
        "    \n",
        "#     df_combined_matches = df_combined_matches.drop('level_1', axis=1)\n",
        "\n",
        "#     df_combined_matches = df_combined_matches\\\n",
        "#       .rename(columns={\"level_0\": \"pip_welfare\"})\n",
        "\n",
        "#     # Add in third scenario.\n",
        "#     df_combined_matches = pd.concat([df_combined_matches, scenario_matches[2]])\n",
        "\n",
        "#     # add scenario name to te pip_welfare column\n",
        "#     df_combined_matches['pip_welfare'] = df_combined_matches['pip_welfare'].fillna(df_scenarios[2].name)\n",
        "\n",
        "#     # Keep only one match per group (e.g. per Country) - in the priority laid out in the df_scenarios list above (income only -> consumption only -> mixed)\n",
        "#       # First count the matches\n",
        "#     df_combined_matches['match_count'] = df_combined_matches.groupby(group_by_col)['pip_welfare'].transform('count')\n",
        "#       # Then drop any matches from the lowest priority where there are multiple matches\n",
        "#     df_combined_matches = df_combined_matches.loc[(df_combined_matches['match_count']==1) | ~(df_combined_matches['pip_welfare']==df_scenarios[2].name)]\n",
        "#       #  Repeat at the next level of priority\n",
        "#     df_combined_matches['match_count'] = df_combined_matches.groupby(group_by_col)['pip_welfare'].transform('count')\n",
        "#     df_combined_matches = df_combined_matches.loc[(df_combined_matches['match_count']==1) | ~(df_combined_matches['pip_welfare']==df_scenarios[1].name)]\n",
        "    \n",
        "#     # Drop the match count column\n",
        "#     df_combined_matches = df_combined_matches.drop('match_count', axis=1)\n",
        "\n",
        "#     return df_combined_matches\n",
        "\n",
        "\n",
        "# #  ---------------------------------\n",
        "# #  ---------- Specifications -------------\n",
        "# #  ----------------------------------\n",
        "\n",
        "# # These parameters are not controlled in the app.\n",
        "# reference_col = 'Year'\n",
        "# group_by_col = 'Entity'\n",
        "\n",
        "# tolerance = 1\n",
        "# outlier_cut_off_upper = None\n",
        "\n",
        "# source_count_requirement = 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #  ---------------------------------\n",
        "# #  ---------- App code -------------\n",
        "# #  ----------------------------------\n",
        "\n",
        "# def panel_box(*args, **kwargs):\n",
        "#     return ui.div(\n",
        "#         ui.div(*args, class_=\"card-body\"),\n",
        "#         **kwargs,\n",
        "#         class_=\"card mb-3\",\n",
        "#     )\n",
        "\n",
        "# app_ui = ui.page_fluid(\n",
        "#     {\"class\": \"p-4\"},\n",
        "#     ui.row(\n",
        "#         ui.column(\n",
        "#             4,\n",
        "#             panel_box(\n",
        "#                 ui.input_slider(\"reference_vals\", \"Reference years\", 1975, 2020, value=[1990,2018]),\n",
        "#                 ui.input_numeric(\n",
        "#                             \"max_dist_from_refs\",\n",
        "#                             \"=/- (years)\",\n",
        "#                             4,\n",
        "#                         ),\n",
        "#                 ui.input_numeric(\n",
        "#                             \"min_dist_between\",\n",
        "#                             \"Min distance between observations (years)\",\n",
        "#                             23,\n",
        "#                         ),\n",
        "#                 ui.input_action_button(\n",
        "#                     \"download\", \"Download data and image\", class_=\"btn-primary w-100\"\n",
        "#                 ),\n",
        "#             ),\n",
        "#             ui.navset_tab_card(\n",
        "#                 ui.nav(\n",
        "#                     \"Compare sources\",\n",
        "#                     ui.input_select(\n",
        "#                 \"metric\",\n",
        "#                 \"Metric\",\n",
        "#                 {\n",
        "#                     \"Gini\": \"Gini\",\n",
        "#                     \"Top_10_share\": \"Top 10p.c. share\",\n",
        "#                 },\n",
        "#             ),\n",
        "#             ),\n",
        "#                 ui.nav(\n",
        "#                   \"Compare metrics\"),\n",
        "#             ),\n",
        "#         ),\n",
        "#         ui.column(\n",
        "#             8,\n",
        "#             ui.output_plot(\"example_plot\")\n",
        "#             ),\n",
        "#     ),\n",
        "# )\n",
        "\n",
        "\n",
        "# def server(input, output, session):\n",
        "#     @output\n",
        "#     @render.plot\n",
        "#     def example_plot():\n",
        "    \n",
        "#       url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/pip.csv'\n",
        "        \n",
        "#       df_pip = pd.read_csv(open_url(url))\n",
        "\n",
        "\n",
        "#       url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/wid.csv'\n",
        "\n",
        "#       df_wid = pd.read_csv(open_url(url))\n",
        "\n",
        "#       url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/region_mapping.csv'\n",
        "\n",
        "#       df_regions = pd.read_csv(open_url(url))\n",
        "\n",
        "\n",
        "#       url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/population.csv'\n",
        "\n",
        "#       df_pop = pd.read_csv(open_url(url))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#       def prep_wid_pip_data(reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n",
        "        \n",
        "#         pip_matches = pip_welfare_routine(\n",
        "#           df = df_pip,\n",
        "#           reference_vals = reference_vals,\n",
        "#           max_dist_from_refs = max_dist_from_refs,\n",
        "#           min_dist_between = min_dist_between,\n",
        "#           reference_col = reference_col,\n",
        "#           group_by_col = group_by_col,\n",
        "#           value_col = value_col\n",
        "#         )\n",
        "\n",
        "#         wid_matches = merge_two_ref_matches(\n",
        "#           df = df_wid,\n",
        "#           reference_vals = reference_vals,\n",
        "#           max_dist_from_refs = max_dist_from_refs,\n",
        "#           min_dist_between = min_dist_between,\n",
        "#           reference_col = reference_col,\n",
        "#           group_by_col = group_by_col,\n",
        "#           value_col = value_col\n",
        "#         )\n",
        "\n",
        "#         ref_pairs = pd.concat([pip_matches, wid_matches,], keys=['pip', 'wid'])\n",
        "\n",
        "#         # Tidy up indexes\n",
        "#         ref_pairs = ref_pairs.reset_index()\n",
        "        \n",
        "#         ref_pairs = ref_pairs.drop('level_1', axis=1)\n",
        "\n",
        "#         ref_pairs = ref_pairs\\\n",
        "#           .rename(columns={\"level_0\": \"source\"})\n",
        "\n",
        "#         return ref_pairs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#       def prep_plot_and_tables(\n",
        "#         reference_vals, \n",
        "#         max_dist_from_refs,\n",
        "#         min_dist_between,\n",
        "#         reference_col,\n",
        "#         group_by_col,\n",
        "#         value_col,\n",
        "#         tolerance,\n",
        "#         outlier_cut_off_upper,\n",
        "#         source_count_requirement):\n",
        "\n",
        "#         # Other spec:\n",
        "#         region_col = 'region_alt'\n",
        "\n",
        "#         # ---- Prep data ---------\n",
        "\n",
        "#         plot_data = prep_wid_pip_data(\n",
        "#           reference_vals = reference_vals, \n",
        "#           max_dist_from_refs = max_dist_from_refs,\n",
        "#           min_dist_between = min_dist_between,\n",
        "#           reference_col = reference_col,\n",
        "#           group_by_col = group_by_col,\n",
        "#           value_col = value_col,\n",
        "#         )\n",
        "\n",
        "#         # Store the names of the columns to be used onthe X and Y axis\n",
        "#         x_axis = f'{value_col}{reference_vals[0]}'\n",
        "#         y_axis = f'{value_col}{reference_vals[1]}'\n",
        "\n",
        "#         x_ref_val = f'{reference_col}{reference_vals[0]}'\n",
        "#         y_ref_val = f'{reference_col}{reference_vals[1]}'\n",
        "\n",
        "\n",
        "#         # Apply outlier cut off, if specified\n",
        "#         if not pd.isna(outlier_cut_off_upper):\n",
        "#           plot_data = plot_data.loc[plot_data[x_axis] <= outlier_cut_off_upper]\n",
        "#           plot_data = plot_data.loc[plot_data[y_axis] <= outlier_cut_off_upper]\n",
        "\n",
        "\n",
        "#         # Add a count by country – showing whether data is available from both sources or not\n",
        "#         plot_data['source_count'] = plot_data.groupby(group_by_col)['source'].transform('count')\n",
        "\n",
        "#         # Apply source requirement (whether to include only observations with data from both sources)\n",
        "#         plot_data = plot_data.loc[plot_data['source_count'] >= source_count_requirement]\n",
        "\n",
        "#         # Drop source_count column\n",
        "#         plot_data = plot_data.drop('source_count', axis=1)\n",
        "\n",
        "#         # Add in region classification\n",
        "#         plot_data = pd.merge(plot_data, df_regions, how = 'left')\n",
        "\n",
        "#         # Add in population data\n",
        "#           # For first ref\n",
        "#         df_pop_ref = df_pop.loc[df_pop['Year'] == reference_vals[0], ['Entity', 'population'] ]\n",
        "\n",
        "#         plot_data = pd.merge(plot_data, df_pop_ref, how = 'left')\n",
        "\n",
        "#         plot_data = plot_data.rename(columns={'population':'population_ref1'})\n",
        "\n",
        "#           # For second ref\n",
        "#         df_pop_ref = df_pop.loc[df_pop['Year'] == reference_vals[1], ['Entity', 'population'] ]\n",
        "\n",
        "#         plot_data = pd.merge(plot_data, df_pop_ref, how = 'left')\n",
        "\n",
        "#         plot_data = plot_data.rename(columns={'population':'population_ref2'})\n",
        "\n",
        "#           # Calculate average population two reference periods\n",
        "#         plot_data['avg_pop'] = (plot_data['population_ref1'] + plot_data['population_ref2'])/2\n",
        "\n",
        "\n",
        "\n",
        "#         # ----- Prep plot ------\n",
        "\n",
        "#         # I grab what I am guessing to be the max and min tick marks on the x axist, in order to define the coordinates of a 'tolerance' ribbon.\n",
        "#         x_min = plot_data[x_axis].min()\n",
        "#         x_min_floor = np.floor(x_min * 10) / 10\n",
        "\n",
        "#         x_max = plot_data[x_axis].max()\n",
        "#         x_max_ceiling = np.ceil(x_max * 10) / 10\n",
        "\n",
        "#         # Set the coordinates of the shaded ribbon\n",
        "#         shaded_coord = pd.DataFrame({'x': [x_min_floor, x_max_ceiling, x_max_ceiling, x_min_floor], \n",
        "#           'y': [x_min_floor-tolerance, x_max_ceiling-tolerance, x_max_ceiling+tolerance , x_min_floor+tolerance]})\n",
        "\n",
        "\n",
        "#         plot = (ggplot(plot_data\n",
        "#         , aes(x_axis, y_axis, alpha=0.5))\n",
        "#         + geom_point(aes(color=region_col, size='avg_pop', alpha = 0.6))\n",
        "#         + facet_wrap('~ source')\n",
        "#         + geom_polygon(aes(x='x', y='y', alpha = 0.2), data=shaded_coord)\n",
        "#         + theme_light()\n",
        "#         + theme(legend_position='none')\n",
        "#         )\n",
        "\n",
        "\n",
        "#         # ---- Prep summary table ----\n",
        "\n",
        "\n",
        "#         # Calculate the change between the two ref periods\n",
        "#         plot_data['change'] = (plot_data[y_axis] - plot_data[x_axis])\n",
        "\n",
        "\n",
        "#         group_yes_vars = ['source', region_col]\n",
        "#         group_no_vars = ['source']\n",
        "\n",
        "#         group_scenarios = [group_no_vars, group_yes_vars] \n",
        "\n",
        "#         aggs = []\n",
        "        \n",
        "#         for group_vars in group_scenarios:\n",
        "        \n",
        "#         # Prepare pop-weighted average change\n",
        "#           agg_level = plot_data.copy()\n",
        "\n",
        "#           # Calulate pop weights by source and region\n",
        "#           agg_level['regional_pop_weights'] = agg_level.groupby(group_vars)['avg_pop'].transform(lambda x: x/x.sum()) \n",
        "\n",
        "#           agg_level['global_pop_weights'] = agg_level.groupby('source')['avg_pop'].transform(lambda x: x/x.sum()) \n",
        "\n",
        "#           # Multiply the change by the pop weights \n",
        "#           agg_level['region_weighted_change'] = agg_level['change'] * agg_level['regional_pop_weights']\n",
        "\n",
        "\n",
        "#         # Prepare fall/stable/rise categories\n",
        "#           agg_level['fall'] = agg_level['change'] < -tolerance\n",
        "\n",
        "#           agg_level['stable'] = (agg_level['change'] <= tolerance) & (agg_level['change'] >= -tolerance)\n",
        "\n",
        "#           agg_level['rise'] = agg_level['change'] > tolerance\n",
        "\n",
        "#           agg_level[['fall', 'stable', 'rise']] = agg_level[['fall', 'stable', 'rise']].astype(int)\n",
        "\n",
        "#           agg_level = agg_level.groupby(group_vars)[['change', 'region_weighted_change', 'global_pop_weights', 'fall', 'stable', 'rise']]\\\n",
        "#             .agg(['sum', 'mean', 'count'])\n",
        "\n",
        "#         # Aggregate by source and region\n",
        "#           if group_vars == group_yes_vars:\n",
        "\n",
        "#             agg_level = agg_level\\\n",
        "#               .unstack().T.reset_index()\n",
        "\n",
        "#           elif group_vars == group_no_vars: \n",
        "\n",
        "#             agg_level = agg_level\\\n",
        "#               .T.reset_index()\n",
        "\n",
        "#             agg_level[region_col] = \"World\"\n",
        "\n",
        "#           aggs.append(agg_level) \n",
        "\n",
        "        \n",
        "#         df_summary = pd.concat(aggs)\n",
        "\n",
        "#           # filter for the aggregations we need and label in the 'summary' column\n",
        "#         df_summary.loc[(df_summary['level_0'] == 'change') & (df_summary['level_1'] == 'mean'), 'summary'] = 'avg change'\n",
        "\n",
        "#         df_summary.loc[(df_summary['level_0'] == 'region_weighted_change') & (df_summary['level_1'] == 'sum'), 'summary'] = 'Regional pop-weighted avg change'\n",
        "\n",
        "#         df_summary.loc[(df_summary['level_0'] == 'global_pop_weights') & (df_summary['level_1'] == 'sum'), 'summary'] = 'Global pop weights'\n",
        "\n",
        "#         df_summary.loc[(df_summary['level_0'] == 'change') & (df_summary['level_1'] == 'count'), 'summary'] = 'n'\n",
        "\n",
        "#         df_summary.loc[(df_summary['level_0'] == 'fall') & (df_summary['level_1'] == 'sum'), 'summary'] = 'fall'\n",
        "\n",
        "#         df_summary.loc[(df_summary['level_0'] == 'stable') & (df_summary['level_1'] == 'sum'), 'summary'] = 'stable'\n",
        "\n",
        "#         df_summary.loc[(df_summary['level_0'] == 'rise') & (df_summary['level_1'] == 'sum'), 'summary'] = 'rise'\n",
        "\n",
        "#         df_summary = df_summary[df_summary['summary'].notnull()]\n",
        "\n",
        "\n",
        "#         df_summary = df_summary.drop(['level_0', 'level_1'], axis = 1)\n",
        "\n",
        "\n",
        "#         df_summary['summary'] = pd.Categorical(df_summary['summary'], categories=['fall', 'stable', 'rise','n', 'avg change', 'Regional pop-weighted avg change', 'Global pop weights'], ordered=True)\n",
        "\n",
        "\n",
        "#         df_summary = df_summary.sort_values([region_col,'summary']).set_index([region_col,'summary'])\n",
        "        \n",
        "        \n",
        "#         # format to show appropriate d.p. I run a loop over each row for want of figuring out a better way!\n",
        "#         repeat_length = 7\n",
        "#         for i in range(0, len(df_summary.index.levels[0].unique())):\n",
        "#           df_summary.iloc[i*repeat_length,:] = df_summary.iloc[i*repeat_length,:].map('{:.0f}'.format)\n",
        "#           df_summary.iloc[1 + i*repeat_length,:] = df_summary.iloc[1 + i*repeat_length,:].map('{:.0f}'.format)\n",
        "#           df_summary.iloc[2 + i*repeat_length,:] = df_summary.iloc[2 + i*repeat_length,:].map('{:.0f}'.format)\n",
        "#           df_summary.iloc[3 + i*repeat_length,:] = df_summary.iloc[3 + i*repeat_length,:].map('{:.0f}'.format)\n",
        "#           df_summary.iloc[4 + i*repeat_length,:] = df_summary.iloc[4 + i*repeat_length,:].map('{:.2f}'.format)\n",
        "#           df_summary.iloc[5 + i*repeat_length,:] = df_summary.iloc[5 + i*repeat_length,:].map('{:.2f}'.format)\n",
        "#           df_summary.iloc[6 + i*repeat_length,:] = df_summary.iloc[6 + i*repeat_length,:].map('{:.2f}'.format)\n",
        "\n",
        "\n",
        "\n",
        "#         # ----- Return outputs ------\n",
        "#         plot_and_tables = {\n",
        "#           \"table_data\": plot_data,\n",
        "#           \"plot\": plot,\n",
        "#           \"summary\": df_summary\n",
        "#         }\n",
        "        \n",
        "#         return plot_and_tables\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "#       plot_and_tables = prep_plot_and_tables(\n",
        "#           reference_vals = list(input.reference_vals()), \n",
        "#           max_dist_from_refs = input.max_dist_from_refs(),\n",
        "#           min_dist_between = input.min_dist_between(),\n",
        "#           reference_col = reference_col,\n",
        "#           group_by_col = group_by_col,\n",
        "#           value_col = input.metric(),\n",
        "#           tolerance = tolerance,\n",
        "#           outlier_cut_off_upper = outlier_cut_off_upper,\n",
        "#           source_count_requirement = source_count_requirement\n",
        "#           )\n",
        "\n",
        "#       plot = plot_and_tables['plot']\n",
        "\n",
        "#       return plot\n",
        "\n",
        "\n",
        "# app = App(app_ui, server)\n",
        "\n",
        "```\n"
      ],
      "id": "f89b6948"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}