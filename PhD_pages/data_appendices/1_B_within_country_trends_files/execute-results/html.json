{
  "hash": "12d8fc85d985b3212cf53100bf7776a7",
  "result": {
    "markdown": "---\ntitle: \"Appendix 1.B: Analysing global trends in within-country inequality\"\nformat: html\nwarning: false\nfilters:\n  - shinylive\n\n---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom plotnine import *\nimport pandas as pd\nimport numpy as np\n```\n:::\n\n\nLoad in the data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# df_wid = pd.read_csv(\"data/clean/wid.csv\")\n# df_pip = pd.read_csv(\"data/clean/pip.csv\")\n# df_regions = pd.read_csv(\"data/clean/WID_region_mapping.csv\")\n# df_pop = pd.read_csv(\"data/clean/population.csv\")\n\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# df_wid = pd.read_csv(\"PhD_pages/data_appendices/data/clean/wid.csv\")\n# df_pip = pd.read_csv(\"PhD_pages/data_appendices/data/clean/pip.csv\")\n# df_regions = pd.read_csv(\"PhD_pages/data_appendices/data/clean/WID_region_mapping.csv\")\n# df_pop = pd.read_csv(\"PhD_pages/data_appendices/data/clean/population.csv\")\n```\n:::\n\n\nA key difficulty of comparing trends is that the data is incomplete coverage. The paper uses two methods to calculate and compare aggregate within-country trends across incomplete data: using reference years and using year fixed effects in a regression.\n\n\n## Method 1: Reference years\n\nI need to allow for the income/consumption issue (currently the data includes both – i.e. multi observations per country year).\n\n\n\n### Mapping data to a reference year\nThis function grabs the observation, by a grouping variable (e.g. by country), for which a reference variable (e.g. year) is closest to a reference value (e.g. 2002).\nYou can specify a maximum distance from the reference value, beyond which no match will be returned.\nBecause there may be tie-breaks – matches equally distant above or below the reference value – there is an argmuent to specify how these tie-breaks should resolved.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#  Function get matching for ref years\n# def closest_to_reference(df, reference_val, max_dist_from_ref, reference_col, group_by_col, value_col, tie_break):\n    \n#     df = df.loc[:, [reference_col, group_by_col, value_col]]\n\n#     # Drop NAs\n#     df = df.dropna()\n\n#     # Calculate absolute distance from reference value\n#     df['ref_diff'] = abs(df[reference_col] - reference_val)\n\n#     # Drop any rows with a distance beyond threshold\n#     if not pd.isna(max_dist_from_ref):\n#       df = df.loc[df['ref_diff'] <= max_dist_from_ref]\n\n#     # Keep closest observation to reference value – including tie-breaks (where there is a match above and below the ref value)\n#     df = df[df.groupby(group_by_col)['ref_diff'].transform('min') == df['ref_diff']].reset_index(drop=True)\n\n#     # Settle tie-breaks\n#     if tie_break == 'below':\n#       df = df[df.groupby(group_by_col)[reference_col].transform('min') == df[reference_col]].reset_index(drop=True)\n      \n#     elif tie_break == 'above':\n#       df = df[df.groupby(group_by_col)[reference_col].transform('max') == df[reference_col]].reset_index(drop=True)\n\n#     df = df.drop('ref_diff', axis=1)\n\n#     return df\n\n```\n:::\n\n\nI test this function here:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# test = closest_to_reference(\n#   df = df_wid, \n#   reference_val = 2002,\n#   max_dist_from_ref = 5,\n#   reference_col = 'Year',\n#   group_by_col = 'Entity',\n#   value_col = 'Gini',\n#   tie_break = 'below'\n#   )\n\n# test.head()\n\n```\n:::\n\n\n### Obtaining a pair of observations\n\nThis function runs the `closest_to_reference` function twice, over a pair of reference values.\nTie-breaks are settled so as to maximise the gap between the two reference values.\nYou can also specify a minimum distance between the two observations (e.g. pairs of matches that fall less than X years apart will be dropped).\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Merge matches for different reference points\n# def merge_two_ref_matches(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n\n#   # Make sure the pair of reference values are in ascending order\n#   reference_vals.sort()\n\n#   # Maximise distance between two refs by settling tie-breaks below the lowest ref and above the highest ref \n\n#   # Find matches for lower reference value\n#   lower_ref_matches = closest_to_reference(df, reference_vals[0], max_dist_from_refs[0], reference_col, group_by_col, value_col, 'below')\n\n#   # Find matches for higher reference value\n#   higher_ref_matches = closest_to_reference(df, reference_vals[1], max_dist_from_refs[1], reference_col, group_by_col, value_col, 'above')\n\n#   # Merge the two sets of matches\n#   merged_df = pd.merge(lower_ref_matches, higher_ref_matches, on=group_by_col, suffixes=(reference_vals[0], reference_vals[1]))\n\n#   # Drop obs that do not have data for both ref values\n#   merged_df = merged_df.dropna()\n\n#   # Drop obs where the matched data does not meet the min distance requirement\n#   if not pd.isna(min_dist_between):\n    \n#     # Store the names of the reference column returned from the two matches\n#     ref_var_high = f'{reference_col}{reference_vals[1]}'\n#     ref_var_low = f'{reference_col}{reference_vals[0]}'\n\n#     # Keep only rows >= to the min distance\n#     merged_df = merged_df.loc[(merged_df[ref_var_high] - merged_df[ref_var_low]) >= min_dist_between, :]\n\n\n#   return merged_df\n\n```\n:::\n\n\nI test this function here:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# test = merge_two_ref_matches(\n#   df = df_wid, \n#   reference_vals = [2000, 2010], \n#   max_dist_from_refs = [5, 4],\n#   min_dist_between = 9,\n#   reference_col = 'Year',\n#   group_by_col = 'Entity',\n#   value_col = 'Gini'\n#   )\n\n# test.head()\n```\n:::\n\n\n#### Allowing for the different welfare concepts in the PIP data\n\nAs noted in .... (add backlink) ... the PIP data contains both income and consumption observations.\nThis function produces matched pairs of observations from that data in which only one pair is selected for each value of the grouping variable (i.e. each country).\nPriority is given to pairs for which both observations relate to income. Second priority is given to pairs where both observations relate to consumption.\nOnly if neither pair is available then it will return a mixed pair of observations if that is available.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# For PIP run this three times – first filtering data for just consumpion only, then with income only, then with a dataset that prefers income over consumption\n# def pip_welfare_routine(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n\n#   # Specify the name of the column in which the income/consumption welfare definition is stored\n#   welfare_colname = 'welfare_type'\n\n#   # Creat dataframes for thee scenarios:\n#   # Scenario 1: only allow income data\n#   df_inc_filter = df.loc[df[welfare_colname] == \"income\", :]\n#   df_inc_filter.name = \"Income\"\n\n#   # Scenario 2: only allow consumption data\n#   df_cons_filter = df.loc[df[welfare_colname] == \"consumption\", :]\n#   df_cons_filter.name = \"Consumption\"\n#   # Scenario 3: allow a mix – dropping consumption data where income data is available in the same year\n#   df_mixed = df.copy()\n\n#   df_mixed['welfare_count'] = df_mixed.groupby([reference_col, group_by_col])[welfare_colname].transform('count')\n\n#   df_mixed = df_mixed.loc[(df_mixed['welfare_count'] == 1) | (df_mixed[welfare_colname] == \"income\")]\n\n#   df_mixed.name = \"Mixed\"\n#   #  Store the scneario dataframes in a list\n#   df_scenarios = [df_inc_filter, df_cons_filter, df_mixed]\n\n#   # Run the matching function on each scenario\n#   scenario_matches = [merge_two_ref_matches(\n#     df_scenario, \n#     reference_vals, \n#     max_dist_from_refs, \n#     min_dist_between, \n#     reference_col, \n#     group_by_col, \n#     value_col) for df_scenario in df_scenarios]\n  \n#   # Combine the first two scenarios.\n#   df_combined_matches = pd.concat([scenario_matches[0], scenario_matches[1]], keys=[df_scenarios[0].name, df_scenarios[1].name])\n\n#   # Tidy up indexes\n#   df_combined_matches = df_combined_matches.reset_index()\n  \n#   df_combined_matches = df_combined_matches.drop('level_1', axis=1)\n\n#   df_combined_matches = df_combined_matches\\\n#     .rename(columns={\"level_0\": \"pip_welfare\"})\n\n#   # Add in third scenario.\n#   df_combined_matches = pd.concat([df_combined_matches, scenario_matches[2]])\n\n#   # add scenario name to te pip_welfare column\n#   df_combined_matches['pip_welfare'] = df_combined_matches['pip_welfare'].fillna(df_scenarios[2].name)\n\n#   # Keep only one match per group (e.g. per Country) - in the priority laid out in the df_scenarios list above (income only -> consumption only -> mixed)\n#     # First count the matches\n#   df_combined_matches['match_count'] = df_combined_matches.groupby(group_by_col)['pip_welfare'].transform('count')\n#     # Then drop any matches from the lowest priority where there are multiple matches\n#   df_combined_matches = df_combined_matches.loc[(df_combined_matches['match_count']==1) | ~(df_combined_matches['pip_welfare']==df_scenarios[2].name)]\n#     #  Repeat at the next level of priority\n#   df_combined_matches['match_count'] = df_combined_matches.groupby(group_by_col)['pip_welfare'].transform('count')\n#   df_combined_matches = df_combined_matches.loc[(df_combined_matches['match_count']==1) | ~(df_combined_matches['pip_welfare']==df_scenarios[1].name)]\n  \n#   # Drop the match count column\n#   df_combined_matches = df_combined_matches.drop('match_count', axis=1)\n\n#   return df_combined_matches\n```\n:::\n\n\nI test this function here:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# test = pip_welfare_routine(\n\n#   df = df_pip,\n#   reference_vals = [1986, 2016], \n#   max_dist_from_refs = [5, 5], \n#   min_dist_between = 30, \n#   reference_col = 'Year',\n#   group_by_col = 'Entity',\n#   value_col = 'Gini'\n\n#   )\n\n# test.head()\n```\n:::\n\n\n### Merging reference year aligned data from WID and PIP datasets\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# def prep_wid_pip_data(reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n#   pip_matches = pip_welfare_routine(\n#     df = df_pip,\n#     reference_vals = reference_vals,\n#     max_dist_from_refs = max_dist_from_refs,\n#     min_dist_between = min_dist_between,\n#     reference_col = reference_col,\n#     group_by_col = group_by_col,\n#     value_col = value_col\n#   )\n\n#   wid_matches = merge_two_ref_matches(\n#     df = df_wid,\n#     reference_vals = reference_vals,\n#     max_dist_from_refs = max_dist_from_refs,\n#     min_dist_between = min_dist_between,\n#     reference_col = reference_col,\n#     group_by_col = group_by_col,\n#     value_col = value_col\n#   )\n\n#   ref_pairs = pd.concat([pip_matches, wid_matches,], keys=['pip', 'wid'])\n\n#   # Tidy up indexes\n#   ref_pairs = ref_pairs.reset_index()\n  \n#   ref_pairs = ref_pairs.drop('level_1', axis=1)\n\n#   ref_pairs = ref_pairs\\\n#     .rename(columns={\"level_0\": \"source\"})\n\n#   return ref_pairs\n```\n:::\n\n\n## Prepare plot and tables\n\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# pd.set_option('display.max_rows', None)\n```\n:::\n\n\n## 1990-2017\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Specifications\n# reference_vals = [1990, 2017]\n# max_dist_from_refs = [4, 4]\n# min_dist_between = 23\n\n\n# # These parameters are unlikely to change:\n# reference_col = 'Year'\n# group_by_col = 'Entity'\n```\n:::\n\n\n::: {.panel-tabset}\n## Gini  \n\n\n\n::: {.panel-tabset}\n## All countries\n\n\n\n::: {.panel-tabset}\n## Plot\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# plot_and_tables['plot']\n```\n:::\n\n\n## Summary\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# plot_and_tables['summary']\n```\n:::\n\n\n## Data\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# plot_data_results = plot_and_tables['table_data']\n# plot_data_results\n```\n:::\n\n\n:::\n\n## Countries with data for both sources\n\n\n\n::: {.panel-tabset}\n## Plot\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# plot_and_tables['plot']\n```\n:::\n\n\n## Summary\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# plot_and_tables['summary']\n```\n:::\n\n\n## Data\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# plot_data_results = plot_and_tables['table_data']\n# plot_data_results\n```\n:::\n\n\n:::\n:::\n\n## Top 10% share\n\n\n\n::: {.panel-tabset}\n## All countries\n\n\n\n::: {.panel-tabset}\n## Plot\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n# plot_and_tables['plot']\n```\n:::\n\n\n## Summary\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n# plot_and_tables['summary']\n```\n:::\n\n\n## Data\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\n# plot_data_results = plot_and_tables['table_data']\n# plot_data_results\n```\n:::\n\n\n:::\n\n## Countries with data for both sources\n\n\n\n::: {.panel-tabset}\n## Plot\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n# plot_and_tables['plot']\n```\n:::\n\n\n## Summary\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n# plot_and_tables['summary']\n```\n:::\n\n\n## Data\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n# plot_data_results = plot_and_tables['table_data']\n# plot_data_results\n```\n:::\n\n\n:::\n\n:::\n\nNext measure\n\n:::\n\nThis first block of code.....\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\n#  ---------------------------------\n#  ---------- SECTION 1: Data prep functions -------------\n#  ----------------------------------\n\n#  Function get matching for ref years\ndef closest_to_reference(df, reference_val, max_dist_from_ref, reference_col, group_by_col, value_col, tie_break):\n  \n  df = df.loc[:, [reference_col, group_by_col, value_col]]\n\n  # Drop NAs\n  df = df.dropna()\n\n  # Calculate absolute distance from reference value\n  df['ref_diff'] = abs(df[reference_col] - reference_val)\n\n  # Drop any rows with a distance beyond threshold\n  if not pd.isna(max_dist_from_ref):\n    df = df.loc[df['ref_diff'] <= max_dist_from_ref]\n\n  # Keep closest observation to reference value – including tie-breaks (where there is a match above and below the ref value)\n  df = df[df.groupby(group_by_col)['ref_diff'].transform('min') == df['ref_diff']].reset_index(drop=True)\n\n  # Settle tie-breaks\n  if tie_break == 'below':\n    df = df[df.groupby(group_by_col)[reference_col].transform('min') == df[reference_col]].reset_index(drop=True)\n    \n  elif tie_break == 'above':\n    df = df[df.groupby(group_by_col)[reference_col].transform('max') == df[reference_col]].reset_index(drop=True)\n\n  df = df.drop('ref_diff', axis=1)\n\n  return df\n\n\n\n\n# Merge matches for different reference points\ndef merge_two_ref_matches(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n\n# Make sure the pair of reference values are in ascending order\n  reference_vals.sort()\n\n# Maximise distance between two refs by settling tie-breaks below the lowest ref and above the highest ref \n\n# Find matches for lower reference value\n  lower_ref_matches = closest_to_reference(df, reference_vals[0], max_dist_from_refs, reference_col, group_by_col, value_col, 'below')\n\n# Find matches for higher reference value\n  higher_ref_matches = closest_to_reference(df, reference_vals[1], max_dist_from_refs, reference_col, group_by_col, value_col, 'above')\n\n# Merge the two sets of matches\n  merged_df = pd.merge(lower_ref_matches, higher_ref_matches, on=group_by_col, suffixes=(reference_vals[0], reference_vals[1]))\n\n# Drop obs that do not have data for both ref values\n  merged_df = merged_df.dropna()\n\n# Drop obs where the matched data does not meet the min distance requirement\n  if not pd.isna(min_dist_between):\n  \n  # Store the names of the reference column returned from the two matches\n    ref_var_high = f'{reference_col}{reference_vals[1]}'\n    ref_var_low = f'{reference_col}{reference_vals[0]}'\n\n  # Keep only rows >= to the min distance\n    merged_df = merged_df.loc[(merged_df[ref_var_high] - merged_df[ref_var_low]) >= min_dist_between, :]\n\n\n  return merged_df\n\n\n\n  # For PIP run this three times – first filtering data for just consumpion only, then with income only, then with a dataset that prefers income over consumption\ndef pip_welfare_routine(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n\n    # Specify the name of the column in which the income/consumption welfare definition is stored\n    welfare_colname = 'welfare_type'\n\n    # Creat dataframes for thee scenarios:\n    # Scenario 1: only allow income data\n    df_inc_filter = df.loc[df[welfare_colname] == \"income\", :]\n    df_inc_filter.name = \"Income\"\n\n    # Scenario 2: only allow consumption data\n    df_cons_filter = df.loc[df[welfare_colname] == \"consumption\", :]\n    df_cons_filter.name = \"Consumption\"\n    # Scenario 3: allow a mix – dropping consumption data where income data is available in the same year\n    df_mixed = df.copy()\n\n    df_mixed['welfare_count'] = df_mixed.groupby([reference_col, group_by_col])[welfare_colname].transform('count')\n\n    df_mixed = df_mixed.loc[(df_mixed['welfare_count'] == 1) | (df_mixed[welfare_colname] == \"income\")]\n\n    df_mixed.name = \"Mixed\"\n    #  Store the scneario dataframes in a list\n    df_scenarios = [df_inc_filter, df_cons_filter, df_mixed]\n\n    # Run the matching function on each scenario\n    scenario_matches = [merge_two_ref_matches(\n      df_scenario, \n      reference_vals, \n      max_dist_from_refs, \n      min_dist_between, \n      reference_col, \n      group_by_col, \n      value_col) for df_scenario in df_scenarios]\n    \n    # Combine the first two scenarios.\n    df_combined_matches = pd.concat([scenario_matches[0], scenario_matches[1]], keys=[df_scenarios[0].name, df_scenarios[1].name])\n\n    # Tidy up indexes\n    df_combined_matches = df_combined_matches.reset_index()\n    \n    df_combined_matches = df_combined_matches.drop('level_1', axis=1)\n\n    df_combined_matches = df_combined_matches\\\n      .rename(columns={\"level_0\": \"pip_welfare\"})\n\n    # Add in third scenario.\n    df_combined_matches = pd.concat([df_combined_matches, scenario_matches[2]])\n\n    # add scenario name to te pip_welfare column\n    df_combined_matches['pip_welfare'] = df_combined_matches['pip_welfare'].fillna(df_scenarios[2].name)\n\n    # Keep only one match per group (e.g. per Country) - in the priority laid out in the df_scenarios list above (income only -> consumption only -> mixed)\n      # First count the matches\n    df_combined_matches['match_count'] = df_combined_matches.groupby(group_by_col)['pip_welfare'].transform('count')\n      # Then drop any matches from the lowest priority where there are multiple matches\n    df_combined_matches = df_combined_matches.loc[(df_combined_matches['match_count']==1) | ~(df_combined_matches['pip_welfare']==df_scenarios[2].name)]\n      #  Repeat at the next level of priority\n    df_combined_matches['match_count'] = df_combined_matches.groupby(group_by_col)['pip_welfare'].transform('count')\n    df_combined_matches = df_combined_matches.loc[(df_combined_matches['match_count']==1) | ~(df_combined_matches['pip_welfare']==df_scenarios[1].name)]\n    \n    # Drop the match count column\n    df_combined_matches = df_combined_matches.drop('match_count', axis=1)\n\n    return df_combined_matches\n\n```\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\n#  ---------------------------------\n#  ---------- SECTION 2: Specifications -------------\n#  ----------------------------------\n\n# These parameters are not controlled in the app.\nreference_col = 'Year'\ngroup_by_col = 'Entity'\n\ntolerance = 1\noutlier_cut_off_upper = None\n\nsource_count_requirement = 2\n\n\n\nreference_vals = [1990,2018]\nmax_dist_from_refs = 4\nmin_dist_between = 5\nvalue_col = \"Gini\"\n\n```\n:::\n\n\nThis second block of code we read in the data (this needs to be done in a different way in )\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n#  ---------------------------------\n#  ---------- SECTION 3: Read in data -------------\n#  ----------------------------------\n\n\nurl = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/pip.csv'\n        \ndf_pip = pd.read_csv(url)\n\n\nurl = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/wid.csv'\n\ndf_wid = pd.read_csv(url)\n\nurl = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/WID_region_mapping.csv'\n\ndf_regions = pd.read_csv(url)\n\n\nurl = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/population.csv'\n\ndf_pop = pd.read_csv(url)\n\n```\n:::\n\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\n#  ---------------------------------\n#  ---------- SECTION 4: Run data functions and preoduce plot -------------\n#  ----------------------------------\n\ndef prep_wid_pip_data(reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n  \n  pip_matches = pip_welfare_routine(\n    df = df_pip,\n    reference_vals = reference_vals,\n    max_dist_from_refs = max_dist_from_refs,\n    min_dist_between = min_dist_between,\n    reference_col = reference_col,\n    group_by_col = group_by_col,\n    value_col = value_col\n  )\n\n  wid_matches = merge_two_ref_matches(\n    df = df_wid,\n    reference_vals = reference_vals,\n    max_dist_from_refs = max_dist_from_refs,\n    min_dist_between = min_dist_between,\n    reference_col = reference_col,\n    group_by_col = group_by_col,\n    value_col = value_col\n  )\n\n  ref_pairs = pd.concat([pip_matches, wid_matches,], keys=['pip', 'wid'])\n\n  # Tidy up indexes\n  ref_pairs = ref_pairs.reset_index()\n  \n  ref_pairs = ref_pairs.drop('level_1', axis=1)\n\n  ref_pairs = ref_pairs\\\n    .rename(columns={\"level_0\": \"source\"})\n\n  return ref_pairs\n\n\n\n\ndef prep_plot_and_tables(\n  reference_vals, \n  max_dist_from_refs,\n  min_dist_between,\n  reference_col,\n  group_by_col,\n  value_col,\n  tolerance,\n  outlier_cut_off_upper,\n  source_count_requirement):\n\n  # Other spec:\n  region_col = 'region_alt'\n\n  # ---- Prep data ---------\n\n  plot_data = prep_wid_pip_data(\n    reference_vals = reference_vals, \n    max_dist_from_refs = max_dist_from_refs,\n    min_dist_between = min_dist_between,\n    reference_col = reference_col,\n    group_by_col = group_by_col,\n    value_col = value_col,\n  )\n\n  # Store the names of the columns to be used onthe X and Y axis\n  x_axis = f'{value_col}{reference_vals[0]}'\n  y_axis = f'{value_col}{reference_vals[1]}'\n\n  x_ref_val = f'{reference_col}{reference_vals[0]}'\n  y_ref_val = f'{reference_col}{reference_vals[1]}'\n\n\n  # Apply outlier cut off, if specified\n  if not pd.isna(outlier_cut_off_upper):\n    plot_data = plot_data.loc[plot_data[x_axis] <= outlier_cut_off_upper]\n    plot_data = plot_data.loc[plot_data[y_axis] <= outlier_cut_off_upper]\n\n\n  # Add a count by country – showing whether data is available from both sources or not\n  plot_data['source_count'] = plot_data.groupby(group_by_col)['source'].transform('count')\n\n  # Apply source requirement (whether to include only observations with data from both sources)\n  plot_data = plot_data.loc[plot_data['source_count'] >= source_count_requirement]\n\n  # Drop source_count column\n  plot_data = plot_data.drop('source_count', axis=1)\n\n  # Add in region classification\n  plot_data = pd.merge(plot_data, df_regions, how = 'left')\n\n  # Add in population data\n    # For first ref\n  df_pop_ref = df_pop.loc[df_pop['Year'] == reference_vals[0], ['Entity', 'population'] ]\n\n  plot_data = pd.merge(plot_data, df_pop_ref, how = 'left')\n\n  plot_data = plot_data.rename(columns={'population':'population_ref1'})\n\n    # For second ref\n  df_pop_ref = df_pop.loc[df_pop['Year'] == reference_vals[1], ['Entity', 'population'] ]\n\n  plot_data = pd.merge(plot_data, df_pop_ref, how = 'left')\n\n  plot_data = plot_data.rename(columns={'population':'population_ref2'})\n\n    # Calculate average population two reference periods\n  plot_data['avg_pop'] = (plot_data['population_ref1'] + plot_data['population_ref2'])/2\n\n\n\n  # ----- Prep plot ------\n\n  # I grab what I am guessing to be the max and min tick marks on the x axist, in order to define the coordinates of a 'tolerance' ribbon.\n  x_min = plot_data[x_axis].min()\n  x_min_floor = np.floor(x_min * 10) / 10\n\n  x_max = plot_data[x_axis].max()\n  x_max_ceiling = np.ceil(x_max * 10) / 10\n\n  # Set the coordinates of the shaded ribbon\n  shaded_coord = pd.DataFrame({'x': [x_min_floor, x_max_ceiling, x_max_ceiling, x_min_floor], \n    'y': [x_min_floor-tolerance, x_max_ceiling-tolerance, x_max_ceiling+tolerance , x_min_floor+tolerance]})\n\n\n  plot = (ggplot(plot_data\n  , aes(x_axis, y_axis, alpha=0.5))\n  + geom_point(aes(color=region_col, size='avg_pop', alpha = 0.6))\n  + facet_wrap('~ source')\n  + geom_polygon(aes(x='x', y='y', alpha = 0.2), data=shaded_coord)\n  + theme_light()\n  + theme(legend_position='none')\n  )\n\n\n  # ---- Prep summary table ----\n\n\n  # Calculate the change between the two ref periods\n  plot_data['change'] = (plot_data[y_axis] - plot_data[x_axis])\n\n\n  group_yes_vars = ['source', region_col]\n  group_no_vars = ['source']\n\n  group_scenarios = [group_no_vars, group_yes_vars] \n\n  aggs = []\n  \n  for group_vars in group_scenarios:\n  \n  # Prepare pop-weighted average change\n    agg_level = plot_data.copy()\n\n    # Calulate pop weights by source and region\n    agg_level['regional_pop_weights'] = agg_level.groupby(group_vars)['avg_pop'].transform(lambda x: x/x.sum()) \n\n    agg_level['global_pop_weights'] = agg_level.groupby('source')['avg_pop'].transform(lambda x: x/x.sum()) \n\n    # Multiply the change by the pop weights \n    agg_level['region_weighted_change'] = agg_level['change'] * agg_level['regional_pop_weights']\n\n\n  # Prepare fall/stable/rise categories\n    agg_level['fall'] = agg_level['change'] < -tolerance\n\n    agg_level['stable'] = (agg_level['change'] <= tolerance) & (agg_level['change'] >= -tolerance)\n\n    agg_level['rise'] = agg_level['change'] > tolerance\n\n    agg_level[['fall', 'stable', 'rise']] = agg_level[['fall', 'stable', 'rise']].astype(int)\n\n    agg_level = agg_level.groupby(group_vars)[['change', 'region_weighted_change', 'global_pop_weights', 'fall', 'stable', 'rise']]\\\n      .agg(['sum', 'mean', 'count'])\n\n  # Aggregate by source and region\n    if group_vars == group_yes_vars:\n\n      agg_level = agg_level\\\n        .unstack().T.reset_index()\n\n    elif group_vars == group_no_vars: \n\n      agg_level = agg_level\\\n        .T.reset_index()\n\n      agg_level[region_col] = \"World\"\n\n    aggs.append(agg_level) \n\n  \n  df_summary = pd.concat(aggs)\n\n    # filter for the aggregations we need and label in the 'summary' column\n  df_summary.loc[(df_summary['level_0'] == 'change') & (df_summary['level_1'] == 'mean'), 'summary'] = 'avg change'\n\n  df_summary.loc[(df_summary['level_0'] == 'region_weighted_change') & (df_summary['level_1'] == 'sum'), 'summary'] = 'Regional pop-weighted avg change'\n\n  df_summary.loc[(df_summary['level_0'] == 'global_pop_weights') & (df_summary['level_1'] == 'sum'), 'summary'] = 'Global pop weights'\n\n  df_summary.loc[(df_summary['level_0'] == 'change') & (df_summary['level_1'] == 'count'), 'summary'] = 'n'\n\n  df_summary.loc[(df_summary['level_0'] == 'fall') & (df_summary['level_1'] == 'sum'), 'summary'] = 'fall'\n\n  df_summary.loc[(df_summary['level_0'] == 'stable') & (df_summary['level_1'] == 'sum'), 'summary'] = 'stable'\n\n  df_summary.loc[(df_summary['level_0'] == 'rise') & (df_summary['level_1'] == 'sum'), 'summary'] = 'rise'\n\n  df_summary = df_summary[df_summary['summary'].notnull()]\n\n\n  df_summary = df_summary.drop(['level_0', 'level_1'], axis = 1)\n\n\n  df_summary['summary'] = pd.Categorical(df_summary['summary'], categories=['fall', 'stable', 'rise','n', 'avg change', 'Regional pop-weighted avg change', 'Global pop weights'], ordered=True)\n\n\n  df_summary = df_summary.sort_values([region_col,'summary']).set_index([region_col,'summary'])\n  \n  \n  # format to show appropriate d.p. I run a loop over each row for want of figuring out a better way!\n  repeat_length = 7\n  for i in range(0, len(df_summary.index.levels[0].unique())):\n    df_summary.iloc[i*repeat_length,:] = df_summary.iloc[i*repeat_length,:].map('{:.0f}'.format)\n    df_summary.iloc[1 + i*repeat_length,:] = df_summary.iloc[1 + i*repeat_length,:].map('{:.0f}'.format)\n    df_summary.iloc[2 + i*repeat_length,:] = df_summary.iloc[2 + i*repeat_length,:].map('{:.0f}'.format)\n    df_summary.iloc[3 + i*repeat_length,:] = df_summary.iloc[3 + i*repeat_length,:].map('{:.0f}'.format)\n    df_summary.iloc[4 + i*repeat_length,:] = df_summary.iloc[4 + i*repeat_length,:].map('{:.2f}'.format)\n    df_summary.iloc[5 + i*repeat_length,:] = df_summary.iloc[5 + i*repeat_length,:].map('{:.2f}'.format)\n    df_summary.iloc[6 + i*repeat_length,:] = df_summary.iloc[6 + i*repeat_length,:].map('{:.2f}'.format)\n\n\n\n  # ----- Return outputs ------\n  plot_and_tables = {\n    \"table_data\": plot_data,\n    \"plot\": plot,\n    \"summary\": df_summary\n  }\n  \n  return plot_and_tables\n\n```\n:::\n\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nplot_and_tables = prep_plot_and_tables(\n    reference_vals = reference_vals, \n    max_dist_from_refs = max_dist_from_refs,\n    min_dist_between = min_dist_between,\n    reference_col = reference_col,\n    group_by_col = group_by_col,\n    value_col = value_col,\n    tolerance = tolerance,\n    outlier_cut_off_upper = outlier_cut_off_upper,\n    source_count_requirement = source_count_requirement\n    )\n\nplot = plot_and_tables['plot']\n\n```\n:::\n\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nplot\n```\n\n::: {.cell-output .cell-output-display}\n![](1_B_within_country_trends_files/figure-html/cell-38-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=37}\n```\n<ggplot: (375222525)>\n```\n:::\n:::\n\n\n## Explore the date\n\nI plan to build a Shiny app to help compare trends across datasets (using Shinylive – built on Shiny for Python - ).\n\nHere is a test app just so I can test the wiring of how such an app works.\n\n\n\n\n\n```{shinylive-python}\n#| standalone: true\n#| viewerHeight: 420\n\nfrom shiny import *\nfrom plotnine import *\nfrom pyodide.http import open_url\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n#  ---------------------------------\n#  ---------- Data prep functions -------------\n#  ----------------------------------\n\n#  Function get matching for ref years\ndef closest_to_reference(df, reference_val, max_dist_from_ref, reference_col, group_by_col, value_col, tie_break):\n  \n  df = df.loc[:, [reference_col, group_by_col, value_col]]\n\n  # Drop NAs\n  df = df.dropna()\n\n  # Calculate absolute distance from reference value\n  df['ref_diff'] = abs(df[reference_col] - reference_val)\n\n  # Drop any rows with a distance beyond threshold\n  if not pd.isna(max_dist_from_ref):\n    df = df.loc[df['ref_diff'] <= max_dist_from_ref]\n\n  # Keep closest observation to reference value – including tie-breaks (where there is a match above and below the ref value)\n  df = df[df.groupby(group_by_col)['ref_diff'].transform('min') == df['ref_diff']].reset_index(drop=True)\n\n  # Settle tie-breaks\n  if tie_break == 'below':\n    df = df[df.groupby(group_by_col)[reference_col].transform('min') == df[reference_col]].reset_index(drop=True)\n    \n  elif tie_break == 'above':\n    df = df[df.groupby(group_by_col)[reference_col].transform('max') == df[reference_col]].reset_index(drop=True)\n\n  df = df.drop('ref_diff', axis=1)\n\n  return df\n\n\n\n\n# Merge matches for different reference points\ndef merge_two_ref_matches(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n\n# Make sure the pair of reference values are in ascending order\n  reference_vals.sort()\n\n# Maximise distance between two refs by settling tie-breaks below the lowest ref and above the highest ref \n\n# Find matches for lower reference value\n  lower_ref_matches = closest_to_reference(df, reference_vals[0], max_dist_from_refs, reference_col, group_by_col, value_col, 'below')\n\n# Find matches for higher reference value\n  higher_ref_matches = closest_to_reference(df, reference_vals[1], max_dist_from_refs, reference_col, group_by_col, value_col, 'above')\n\n# Merge the two sets of matches\n  merged_df = pd.merge(lower_ref_matches, higher_ref_matches, on=group_by_col, suffixes=(reference_vals[0], reference_vals[1]))\n\n# Drop obs that do not have data for both ref values\n  merged_df = merged_df.dropna()\n\n# Drop obs where the matched data does not meet the min distance requirement\n  if not pd.isna(min_dist_between):\n  \n  # Store the names of the reference column returned from the two matches\n    ref_var_high = f'{reference_col}{reference_vals[1]}'\n    ref_var_low = f'{reference_col}{reference_vals[0]}'\n\n  # Keep only rows >= to the min distance\n    merged_df = merged_df.loc[(merged_df[ref_var_high] - merged_df[ref_var_low]) >= min_dist_between, :]\n\n\n  return merged_df\n\n\n\n  # For PIP run this three times – first filtering data for just consumpion only, then with income only, then with a dataset that prefers income over consumption\ndef pip_welfare_routine(df, reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n\n    # Specify the name of the column in which the income/consumption welfare definition is stored\n    welfare_colname = 'welfare_type'\n\n    # Creat dataframes for thee scenarios:\n    # Scenario 1: only allow income data\n    df_inc_filter = df.loc[df[welfare_colname] == \"income\", :]\n    df_inc_filter.name = \"Income\"\n\n    # Scenario 2: only allow consumption data\n    df_cons_filter = df.loc[df[welfare_colname] == \"consumption\", :]\n    df_cons_filter.name = \"Consumption\"\n    # Scenario 3: allow a mix – dropping consumption data where income data is available in the same year\n    df_mixed = df.copy()\n\n    df_mixed['welfare_count'] = df_mixed.groupby([reference_col, group_by_col])[welfare_colname].transform('count')\n\n    df_mixed = df_mixed.loc[(df_mixed['welfare_count'] == 1) | (df_mixed[welfare_colname] == \"income\")]\n\n    df_mixed.name = \"Mixed\"\n    #  Store the scneario dataframes in a list\n    df_scenarios = [df_inc_filter, df_cons_filter, df_mixed]\n\n    # Run the matching function on each scenario\n    scenario_matches = [merge_two_ref_matches(\n      df_scenario, \n      reference_vals, \n      max_dist_from_refs, \n      min_dist_between, \n      reference_col, \n      group_by_col, \n      value_col) for df_scenario in df_scenarios]\n    \n    # Combine the first two scenarios.\n    df_combined_matches = pd.concat([scenario_matches[0], scenario_matches[1]], keys=[df_scenarios[0].name, df_scenarios[1].name])\n\n    # Tidy up indexes\n    df_combined_matches = df_combined_matches.reset_index()\n    \n    df_combined_matches = df_combined_matches.drop('level_1', axis=1)\n\n    df_combined_matches = df_combined_matches\\\n      .rename(columns={\"level_0\": \"pip_welfare\"})\n\n    # Add in third scenario.\n    df_combined_matches = pd.concat([df_combined_matches, scenario_matches[2]])\n\n    # add scenario name to te pip_welfare column\n    df_combined_matches['pip_welfare'] = df_combined_matches['pip_welfare'].fillna(df_scenarios[2].name)\n\n    # Keep only one match per group (e.g. per Country) - in the priority laid out in the df_scenarios list above (income only -> consumption only -> mixed)\n      # First count the matches\n    df_combined_matches['match_count'] = df_combined_matches.groupby(group_by_col)['pip_welfare'].transform('count')\n      # Then drop any matches from the lowest priority where there are multiple matches\n    df_combined_matches = df_combined_matches.loc[(df_combined_matches['match_count']==1) | ~(df_combined_matches['pip_welfare']==df_scenarios[2].name)]\n      #  Repeat at the next level of priority\n    df_combined_matches['match_count'] = df_combined_matches.groupby(group_by_col)['pip_welfare'].transform('count')\n    df_combined_matches = df_combined_matches.loc[(df_combined_matches['match_count']==1) | ~(df_combined_matches['pip_welfare']==df_scenarios[1].name)]\n    \n    # Drop the match count column\n    df_combined_matches = df_combined_matches.drop('match_count', axis=1)\n\n    return df_combined_matches\n\n\n#  ---------------------------------\n#  ---------- Specifications -------------\n#  ----------------------------------\n\n# These parameters are not controlled in the app.\nreference_col = 'Year'\ngroup_by_col = 'Entity'\n\ntolerance = 1\noutlier_cut_off_upper = None\n\nsource_count_requirement = 2\n\n\n\n\n\n#  ---------------------------------\n#  ---------- App code -------------\n#  ----------------------------------\n\ndef panel_box(*args, **kwargs):\n    return ui.div(\n        ui.div(*args, class_=\"card-body\"),\n        **kwargs,\n        class_=\"card mb-3\",\n    )\n\napp_ui = ui.page_fluid(\n    {\"class\": \"p-4\"},\n    ui.row(\n        ui.column(\n            4,\n            panel_box(\n                ui.input_slider(\"reference_vals\", \"Reference years\", 1975, 2020, value=[1990,2018]),\n                ui.input_numeric(\n                            \"max_dist_from_refs\",\n                            \"=/- (years)\",\n                            4,\n                        ),\n                ui.input_numeric(\n                            \"min_dist_between\",\n                            \"Min distance between observations (years)\",\n                            23,\n                        ),\n                ui.input_action_button(\n                    \"download\", \"Download data and image\", class_=\"btn-primary w-100\"\n                ),\n            ),\n            ui.navset_tab_card(\n                ui.nav(\n                    \"Compare sources\",\n                    ui.input_select(\n                \"metric\",\n                \"Metric\",\n                {\n                    \"Gini\": \"Gini\",\n                    \"Top_10_share\": \"Top 10p.c. share\",\n                },\n            ),\n            ),\n                ui.nav(\n                  \"Compare metrics\"),\n            ),\n        ),\n        ui.column(\n            8,\n            ui.output_plot(\"example_plot\")\n            ),\n    ),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render.plot\n    def example_plot():\n    \n      url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/pip.csv'\n        \n      df_pip = pd.read_csv(open_url(url))\n\n\n      url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/wid.csv'\n\n      df_wid = pd.read_csv(open_url(url))\n\n      url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/WID_region_mapping.csv'\n\n      df_regions = pd.read_csv(open_url(url))\n\n\n      url = 'https://raw.githubusercontent.com/JoeHasell/personal_site/main/PhD_pages/data_appendices/data/clean/population.csv'\n\n      df_pop = pd.read_csv(open_url(url))\n\n\n\n\n\n\n\n\n\n\n\n\n\n      def prep_wid_pip_data(reference_vals, max_dist_from_refs, min_dist_between, reference_col, group_by_col, value_col):\n        \n        pip_matches = pip_welfare_routine(\n          df = df_pip,\n          reference_vals = reference_vals,\n          max_dist_from_refs = max_dist_from_refs,\n          min_dist_between = min_dist_between,\n          reference_col = reference_col,\n          group_by_col = group_by_col,\n          value_col = value_col\n        )\n\n        wid_matches = merge_two_ref_matches(\n          df = df_wid,\n          reference_vals = reference_vals,\n          max_dist_from_refs = max_dist_from_refs,\n          min_dist_between = min_dist_between,\n          reference_col = reference_col,\n          group_by_col = group_by_col,\n          value_col = value_col\n        )\n\n        ref_pairs = pd.concat([pip_matches, wid_matches,], keys=['pip', 'wid'])\n\n        # Tidy up indexes\n        ref_pairs = ref_pairs.reset_index()\n        \n        ref_pairs = ref_pairs.drop('level_1', axis=1)\n\n        ref_pairs = ref_pairs\\\n          .rename(columns={\"level_0\": \"source\"})\n\n        return ref_pairs\n\n\n\n\n      def prep_plot_and_tables(\n        reference_vals, \n        max_dist_from_refs,\n        min_dist_between,\n        reference_col,\n        group_by_col,\n        value_col,\n        tolerance,\n        outlier_cut_off_upper,\n        source_count_requirement):\n\n        # Other spec:\n        region_col = 'region_alt'\n\n        # ---- Prep data ---------\n\n        plot_data = prep_wid_pip_data(\n          reference_vals = reference_vals, \n          max_dist_from_refs = max_dist_from_refs,\n          min_dist_between = min_dist_between,\n          reference_col = reference_col,\n          group_by_col = group_by_col,\n          value_col = value_col,\n        )\n\n        # Store the names of the columns to be used onthe X and Y axis\n        x_axis = f'{value_col}{reference_vals[0]}'\n        y_axis = f'{value_col}{reference_vals[1]}'\n\n        x_ref_val = f'{reference_col}{reference_vals[0]}'\n        y_ref_val = f'{reference_col}{reference_vals[1]}'\n\n\n        # Apply outlier cut off, if specified\n        if not pd.isna(outlier_cut_off_upper):\n          plot_data = plot_data.loc[plot_data[x_axis] <= outlier_cut_off_upper]\n          plot_data = plot_data.loc[plot_data[y_axis] <= outlier_cut_off_upper]\n\n\n        # Add a count by country – showing whether data is available from both sources or not\n        plot_data['source_count'] = plot_data.groupby(group_by_col)['source'].transform('count')\n\n        # Apply source requirement (whether to include only observations with data from both sources)\n        plot_data = plot_data.loc[plot_data['source_count'] >= source_count_requirement]\n\n        # Drop source_count column\n        plot_data = plot_data.drop('source_count', axis=1)\n\n        # Add in region classification\n        plot_data = pd.merge(plot_data, df_regions, how = 'left')\n\n        # Add in population data\n          # For first ref\n        df_pop_ref = df_pop.loc[df_pop['Year'] == reference_vals[0], ['Entity', 'population'] ]\n\n        plot_data = pd.merge(plot_data, df_pop_ref, how = 'left')\n\n        plot_data = plot_data.rename(columns={'population':'population_ref1'})\n\n          # For second ref\n        df_pop_ref = df_pop.loc[df_pop['Year'] == reference_vals[1], ['Entity', 'population'] ]\n\n        plot_data = pd.merge(plot_data, df_pop_ref, how = 'left')\n\n        plot_data = plot_data.rename(columns={'population':'population_ref2'})\n\n          # Calculate average population two reference periods\n        plot_data['avg_pop'] = (plot_data['population_ref1'] + plot_data['population_ref2'])/2\n\n\n\n        # ----- Prep plot ------\n\n        # I grab what I am guessing to be the max and min tick marks on the x axist, in order to define the coordinates of a 'tolerance' ribbon.\n        x_min = plot_data[x_axis].min()\n        x_min_floor = np.floor(x_min * 10) / 10\n\n        x_max = plot_data[x_axis].max()\n        x_max_ceiling = np.ceil(x_max * 10) / 10\n\n        # Set the coordinates of the shaded ribbon\n        shaded_coord = pd.DataFrame({'x': [x_min_floor, x_max_ceiling, x_max_ceiling, x_min_floor], \n          'y': [x_min_floor-tolerance, x_max_ceiling-tolerance, x_max_ceiling+tolerance , x_min_floor+tolerance]})\n\n\n        plot = (ggplot(plot_data\n        , aes(x_axis, y_axis, alpha=0.5))\n        + geom_point(aes(color=region_col, size='avg_pop', alpha = 0.6))\n        + facet_wrap('~ source')\n        + geom_polygon(aes(x='x', y='y', alpha = 0.2), data=shaded_coord)\n        + theme_light()\n        + theme(legend_position='none')\n        )\n\n\n        # ---- Prep summary table ----\n\n\n        # Calculate the change between the two ref periods\n        plot_data['change'] = (plot_data[y_axis] - plot_data[x_axis])\n\n\n        group_yes_vars = ['source', region_col]\n        group_no_vars = ['source']\n\n        group_scenarios = [group_no_vars, group_yes_vars] \n\n        aggs = []\n        \n        for group_vars in group_scenarios:\n        \n        # Prepare pop-weighted average change\n          agg_level = plot_data.copy()\n\n          # Calulate pop weights by source and region\n          agg_level['regional_pop_weights'] = agg_level.groupby(group_vars)['avg_pop'].transform(lambda x: x/x.sum()) \n\n          agg_level['global_pop_weights'] = agg_level.groupby('source')['avg_pop'].transform(lambda x: x/x.sum()) \n\n          # Multiply the change by the pop weights \n          agg_level['region_weighted_change'] = agg_level['change'] * agg_level['regional_pop_weights']\n\n\n        # Prepare fall/stable/rise categories\n          agg_level['fall'] = agg_level['change'] < -tolerance\n\n          agg_level['stable'] = (agg_level['change'] <= tolerance) & (agg_level['change'] >= -tolerance)\n\n          agg_level['rise'] = agg_level['change'] > tolerance\n\n          agg_level[['fall', 'stable', 'rise']] = agg_level[['fall', 'stable', 'rise']].astype(int)\n\n          agg_level = agg_level.groupby(group_vars)[['change', 'region_weighted_change', 'global_pop_weights', 'fall', 'stable', 'rise']]\\\n            .agg(['sum', 'mean', 'count'])\n\n        # Aggregate by source and region\n          if group_vars == group_yes_vars:\n\n            agg_level = agg_level\\\n              .unstack().T.reset_index()\n\n          elif group_vars == group_no_vars: \n\n            agg_level = agg_level\\\n              .T.reset_index()\n\n            agg_level[region_col] = \"World\"\n\n          aggs.append(agg_level) \n\n        \n        df_summary = pd.concat(aggs)\n\n          # filter for the aggregations we need and label in the 'summary' column\n        df_summary.loc[(df_summary['level_0'] == 'change') & (df_summary['level_1'] == 'mean'), 'summary'] = 'avg change'\n\n        df_summary.loc[(df_summary['level_0'] == 'region_weighted_change') & (df_summary['level_1'] == 'sum'), 'summary'] = 'Regional pop-weighted avg change'\n\n        df_summary.loc[(df_summary['level_0'] == 'global_pop_weights') & (df_summary['level_1'] == 'sum'), 'summary'] = 'Global pop weights'\n\n        df_summary.loc[(df_summary['level_0'] == 'change') & (df_summary['level_1'] == 'count'), 'summary'] = 'n'\n\n        df_summary.loc[(df_summary['level_0'] == 'fall') & (df_summary['level_1'] == 'sum'), 'summary'] = 'fall'\n\n        df_summary.loc[(df_summary['level_0'] == 'stable') & (df_summary['level_1'] == 'sum'), 'summary'] = 'stable'\n\n        df_summary.loc[(df_summary['level_0'] == 'rise') & (df_summary['level_1'] == 'sum'), 'summary'] = 'rise'\n\n        df_summary = df_summary[df_summary['summary'].notnull()]\n\n\n        df_summary = df_summary.drop(['level_0', 'level_1'], axis = 1)\n\n\n        df_summary['summary'] = pd.Categorical(df_summary['summary'], categories=['fall', 'stable', 'rise','n', 'avg change', 'Regional pop-weighted avg change', 'Global pop weights'], ordered=True)\n\n\n        df_summary = df_summary.sort_values([region_col,'summary']).set_index([region_col,'summary'])\n        \n        \n        # format to show appropriate d.p. I run a loop over each row for want of figuring out a better way!\n        repeat_length = 7\n        for i in range(0, len(df_summary.index.levels[0].unique())):\n          df_summary.iloc[i*repeat_length,:] = df_summary.iloc[i*repeat_length,:].map('{:.0f}'.format)\n          df_summary.iloc[1 + i*repeat_length,:] = df_summary.iloc[1 + i*repeat_length,:].map('{:.0f}'.format)\n          df_summary.iloc[2 + i*repeat_length,:] = df_summary.iloc[2 + i*repeat_length,:].map('{:.0f}'.format)\n          df_summary.iloc[3 + i*repeat_length,:] = df_summary.iloc[3 + i*repeat_length,:].map('{:.0f}'.format)\n          df_summary.iloc[4 + i*repeat_length,:] = df_summary.iloc[4 + i*repeat_length,:].map('{:.2f}'.format)\n          df_summary.iloc[5 + i*repeat_length,:] = df_summary.iloc[5 + i*repeat_length,:].map('{:.2f}'.format)\n          df_summary.iloc[6 + i*repeat_length,:] = df_summary.iloc[6 + i*repeat_length,:].map('{:.2f}'.format)\n\n\n\n        # ----- Return outputs ------\n        plot_and_tables = {\n          \"table_data\": plot_data,\n          \"plot\": plot,\n          \"summary\": df_summary\n        }\n        \n        return plot_and_tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      \n\n      plot_and_tables = prep_plot_and_tables(\n          reference_vals = list(input.reference_vals()), \n          max_dist_from_refs = input.max_dist_from_refs(),\n          min_dist_between = input.min_dist_between(),\n          reference_col = reference_col,\n          group_by_col = group_by_col,\n          value_col = input.metric(),\n          tolerance = tolerance,\n          outlier_cut_off_upper = outlier_cut_off_upper,\n          source_count_requirement = source_count_requirement\n          )\n\n      plot = plot_and_tables['plot']\n\n      return plot\n\n\napp = App(app_ui, server)\n\n```\n\n",
    "supporting": [
      "1_B_within_country_trends_files"
    ],
    "filters": [],
    "includes": {}
  }
}